{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 모델을 학습시키기 전에 의사결정\n",
    "\n",
    "1. 어떤 loss를 쓸 것인지\n",
    "2. learning rate은 몇으로 할 것인지\n",
    "3. optimizer는 무엇으로 할 것인지\n",
    "4. 학습 횟수는 몇 번으로 할 것인지\n",
    "\n",
    "#### 기본루틴\n",
    "- *optimizer.zero_grad()*  \n",
    "- *pred = model(x)*  \n",
    "- *loss = criterion(pred, x_labels)*  \n",
    "- *loss.backward()*  \n",
    "- *optimizer.step()*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, X_dim, y_dim):\n",
    "        super(MyModel, self).__init__()\n",
    "        layer1 = nn.Linear(X_dim, 128)\n",
    "        activation1 = nn.ReLU()\n",
    "        layer2 = nn.Linear(128, y_dim)\n",
    "        self.module = nn.Sequential(\n",
    "            layer1,\n",
    "            activation1,\n",
    "            layer2\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.module(x)\n",
    "        result = F.softmax(out, dim=1)\n",
    "        return result        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-dca9b6b8b243>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparaeters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mnum_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# 준비재료\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-5\n",
    "optimizer = optim.SGD(model.paraeters(), lr=learning_rate)\n",
    "num_epochs = 2\n",
    "num_batches = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\tfor i, data in enumerate(train_loader):\n",
    "\t\tx, x_labels = data # x.size() = [batch, channel, x, y]\n",
    "\t\t# init grad\n",
    "\t\toptimizer.zero_grad() # step과 zero_grad는 쌍을 이루는 것이라고 생각하면 됨\n",
    "\t\t# forward\n",
    "\t\tpred = model(x)\n",
    "\t\t# calculate loss\n",
    "\t\tloss = criterion(pred, x_labels)\n",
    "\t\t# backpropagation\n",
    "\t\tloss.backward()\n",
    "\t\t# weight update\n",
    "\t\toptimizer.step()\n",
    "\t\t# 학습과정 출력\n",
    "\t\trunning_loss += loss.item()\n",
    "\t\tif (i+1)%2000 == 0: # print every 2000 mini-batches\n",
    "\t\t\tprint(\"epoch: {}/{} | step: {}/{} | loss: {:.4f}\".format(epoch, num_epochs, i+1, num_batches, running_loss/2000))\n",
    "\t\t\trunning_loss = 0.0\n",
    "\n",
    "print(\"finish Training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어의 표현 방법 https://wikidocs.net/60852\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원핫 인코딩 한계\n",
    " 1. 단어 개수가 늘어날수록 벡터 공간도 함께 늘어남(저장 공간 측면에서 매우 비효율적)\n",
    " 2. 단어의 유사도를 표현하지 못함\n",
    "\n",
    "### 희소 표현(희소 벡터)\n",
    " - 벡터 또는 행렬(matrix)의 값이 대부분 0으로 표현되는 방법을 희소 표현(sparse representation)이라고 함). 원-핫 벡터는 희소 벡터이다(원핫 인코딩의 한계를 그대로 가짐)\n",
    " \n",
    " Ex) 강아지 = [ 0 0 0 0 1 0 0 0 0 0 0 0 ... 중략 ... 0] # 이 때 1 뒤의 0의 수는 9995개. 차원은 10,000 (단어가 10,000개라고 가정 시)\n",
    " \n",
    "### 밀집 표현(밀집 벡터)\n",
    " - 사용자가 밀집 표현의 차원을 128로 설정한다면, 모든 단어의 벡터 표현의 차원은 128로 바뀌면서 모든 값이 실수가 된다.\n",
    " \n",
    " Ex) 강아지 = [0.2 1.8 1.1 -2.1 1.1 2.8 ... 중략 ...] # 이 벡터의 차원은 128\n",
    " \n",
    "### 워드 임베딩\n",
    " - 단어를 밀집 벡터(dense vector)의 형태로 표현하는 방법을 워드 임베딩이라고 함. 그리고 이 밀집 벡터를 워드임베딩 과정을 통해 나온 결과라고 하여 임베딩 벡터라고도 한다.\n",
    " \n",
    " - 워드 임베딩 방법론으로는 LSA, Word2Vec, FastText, Glove 등이 있음 \n",
    " - Pytorch 에서 제공하는 도구인 nn.embedding()는 앞서 언급한 방법들을 사용하지는 않지만, 단어를 랜덤한 값을 가지는 밀집 벡터로 변환한 뒤에, 인공 신경망의 가중치를 학습하는 것과 같은 방식으로 단어 벡터를 학습하는 방법을 사용함\n",
    " \n",
    "### 워드투벡터(Word2Vec)\n",
    " - 단어 간 유사도를 반영할 수 있도록 단어의 의미를 벡터화 하는 방법\n",
    " - 분산 표현 : 단어의 '의미'를 다차원 공간에 벡터화하는 방법\n",
    " - 이렇게 분산 표현을 이용하여 단어의 유사도를 벡터화하는 작업은 워드 임베딩(embedding) 작업에 속하기 때문에 이렇게 표현된 벡터 또한 임베딩 벡터(embedding vector)라고 하며, 저차원을 가지므로 바로 앞의 챕터에서 배운 밀집 벡터(dense vector)에도 속함\n",
    " \n",
    "### 분산표현\n",
    " - 요약하면 희소 표현이 고차원에 각 차원이 분리된 표현 방법이었다면, 분산 표현은 저차원에 단어의 의미를 여러 차원에다가 분산하여 표현한다. 이런 표현 방법을 사용하면 단어 간 유사도를 계산할 수 있다.\n",
    "\n",
    " - 이를 위한 학습 방법으로는 NNLM, RNNLM 등이 있으나 요즘에는 해당 방법들의 속도를 대폭 개선시킨 Word2Vec가 많이 쓰이고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch의 nn.Embedding()\n",
    "- 파이토치에서는 임베딩 벡터를 사용하는 방법이 크게 두 가지가 있다. \n",
    "    - 임베딩 층(embedding layer)을 만들어 훈련 데이터로부터 처음부터 임베딩 벡터를 학습하는 방법\n",
    "    - 미리 사전에 훈련된 임베딩 벡터(pre-trained word embedding)들을 가져와 사용하는 방법\n",
    "    \n",
    "### 임베딩 층은 룩업 테이블이다.\n",
    "- 임베딩 층의 입력으로 사용하기 위해서 입력 시퀀스의 각 단어들은 모두 정수 인코딩이 되어있어야 한다\n",
    "\n",
    "어떤 단어 → 단어에 부여된 고유한 정수값 → 임베딩 층 통과 → 밀집 벡터\n",
    "\n",
    "1) 임베딩 층은 입력 정수에 대해 밀집 벡터(dense vector)로 매핑\n",
    "2) 이 밀집 벡터는 인공 신경망의 학습 과정에서 가중치가 학습되는 것과 같은 방식으로 훈련됨. \n",
    "3) 훈련 과정에서 단어는 모델이 풀고자하는 작업에 맞는 값으로 업데이트 된다. \n",
    "이 밀집 벡터를 임베딩 벡터라고 부른다.\n",
    "    \n",
    "\n",
    "정수를 밀집 벡터 또는 임베딩 벡터로 맵핑한다는 것은 어떤 의미일까요? 특정 단어와 맵핑되는 정수를 인덱스로 가지는 테이블로부터 임베딩 벡터 값을 가져오는 룩업 테이블이라고 볼 수 있습니다. 그리고 이 테이블은 단어 집합의 크기만큼의 행을 가지므로 모든 단어는 고유한 임베딩 벡터를 가집니다.\n",
    "\n",
    "룩업테이블에 있는 단어의 임베딩 벡터들은 역전파 과정에서 값이 학습된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'to': 2, 'code': 3, 'need': 4, 'know': 5, 'you': 6, 'how': 7, '<unk>': 0, '<pad>': 1}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "train_data = 'you need to know how to code'\n",
    "word_set = set(train_data.split()) # 중복을 제거한 단어들의 집합인 단어 집합 생성.\n",
    "vocab = {word: i+2 for i, word in enumerate(word_set)}  # 단어 집합의 각 단어에 고유한 정수 매핑.\n",
    "vocab['<unk>'] = 0\n",
    "vocab['<pad>'] = 1\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 집합의 크기만큼의 행을 가지는 테이블 생성. (임베딩 벡터의 차원은 3이라 가정)\n",
    "embedding_table = torch.FloatTensor([\n",
    "                               [ 0.0,  0.0,  0.0],  # 0\n",
    "                               [ 0.0,  0.0,  0.0],  # 1\n",
    "                               [ 0.2,  0.9,  0.3],  # 2\n",
    "                               [ 0.1,  0.5,  0.7],  # 3\n",
    "                               [ 0.2,  0.1,  0.8],  # 4\n",
    "                               [ 0.4,  0.1,  0.1],  # 5\n",
    "                               [ 0.1,  0.8,  0.9],  # 6\n",
    "                               [ 0.6,  0.1,  0.1]])  # 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1000, 0.8000, 0.9000],\n",
      "        [0.2000, 0.1000, 0.8000],\n",
      "        [0.2000, 0.9000, 0.3000],\n",
      "        [0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# 임의의 샘플 문장\n",
    "sample = 'you need to run'.split()\n",
    "idxes=[]\n",
    "# 각 단어를 정수로 변환\n",
    "for word in sample:\n",
    "  try:\n",
    "    idxes.append(vocab[word])\n",
    "  except KeyError: # 단어 집합에 없는 단어일 경우 <unk>로 대체된다.\n",
    "    idxes.append(vocab['<unk>'])\n",
    "idxes = torch.LongTensor(idxes) # tensor([6, 4, 2, 0])\n",
    "\n",
    "# 룩업 테이블\n",
    "lookup_result = embedding_table[idxes, :] # 각 정수를 인덱스로 임베딩 테이블에서 값을 가져온다.\n",
    "print(lookup_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 임베딩 층 사용하기\n",
    "\n",
    "nn.Embedding은 크게 두 가지 인자를 받는데 각각 num_embeddings과 embedding_dim입니다.\n",
    "\n",
    " - num_embeddings : 임베딩을 할 단어들의 개수. 다시 말해 단어 집합의 크기.\n",
    " - embedding_dim : 임베딩 할 벡터의 차원. 사용자가 정해주는 하이퍼파라미터이다.\n",
    " - padding_idx : 선택적으로 사용하는 인자. 패딩을 위한 토큰의 인덱스를 알려준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = 'you need to know how to code'\n",
    "word_set = set(train_data.split()) # 중복을 제거한 단어들의 집합인 단어 집합 생성.\n",
    "vocab = {tkn: i+2 for i, tkn in enumerate(word_set)}  # 단어 집합의 각 단어에 고유한 정수 맵핑.\n",
    "vocab['<unk>'] = 0\n",
    "vocab['<pad>'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "embedding_layer = nn.Embedding(num_embeddings = len(vocab), \n",
    "                               embedding_dim = 3,\n",
    "                               padding_idx = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 2.8888,  1.5721,  0.5345],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.2195,  1.0877, -0.9658],\n",
      "        [-0.9405,  0.2130,  1.4181],\n",
      "        [ 0.8394,  0.2810,  0.8668],\n",
      "        [ 0.2448, -0.5023,  0.9176],\n",
      "        [-0.8760,  0.8622,  0.7863],\n",
      "        [-0.4443,  0.6083, -0.7428]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#print(embedding_layer)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사전 훈련된 워드 임베딩(Pretrained Word Embedding) : https://wikidocs.net/64904\n",
    " - nn.Embedding()을 사용하는 것보다 다른 텍스트 데이터로 사전 훈련되어 있는 임베딩 벡터를 불러오는 것이 나은 선택일 수 있다.\n",
    " - 훈련 데이터가 적다면 파이토치의 nn.Embedding()으로 해당 문제에 충분히 특화된 임베딩 벡터를 만들어내는 것이 쉽지 않다. 이 경우, 해당 문제에 특화된 것은 아니지만 보다 일반적이고 보다 많은 훈련 데이터로 이미 Word2Vec이나 GloVe 등으로 학습되어져 있는 임베딩 벡터들을 사용하는 것이 성능의 개선을 가져올 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-f8dab9081d39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "from torchtext import data, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
