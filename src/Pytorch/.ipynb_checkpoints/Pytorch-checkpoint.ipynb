{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (핸들링) 텐서 다루기 https://wikidocs.net/52460\n",
    "\n",
    "- 자연어 처리는 보통 (batch size, 문장 길이, 단어 벡터의 차원)이라는 3차원 텐서를 사용함\n",
    "    - 문장 샘플들, 문장 샘플들의 길이(단어 수), 해당 문장에 대한 차원(밀집 벡터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3차원 감각 익히기\n",
    "t = np.array([[[0, 1, 2, 4], [3, 4, 5, 4], [2, 5, 4, 3]],\n",
    "            [[6, 7, 8, 4], [9, 10, 11, 4], [8, 13, 24, 5]]])\n",
    "ft = torch.FloatTensor(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.shape)  # 샘플의 최종 묶음(문장)은 두개, 세개 단어씩, 그리고 각 문장마다 표현할 밀집 벡터. 가장 바깥쪽 괄호부터 하나씩 진입해서 보면 눈에 보인다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) 뷰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4) 뷰 (https://wikidocs.net/52846)\n",
    "t = np.array([[[0, 1, 2],\n",
    "               [3, 4, 5]],\n",
    "              [[6, 7, 8],\n",
    "               [9, 10, 11]]])\n",
    "ft = torch.FloatTensor(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.,  2.],\n",
      "         [ 3.,  4.,  5.]],\n",
      "\n",
      "        [[ 6.,  7.,  8.],\n",
      "         [ 9., 10., 11.]]])\n",
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft)\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.]])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "# 3차원 to 2차원\n",
    "print(ft.view([-1, 3])) # ft라는 텐서를 (?, 3)의 크기로 변경\n",
    "print(ft.view([-1, 3]).shape)  # -1은 파이토치에게 맡기는 것. 결과적으로 (4, 3)의 크기를 가지는 텐서로 처리됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.,  2.]],\n",
      "\n",
      "        [[ 3.,  4.,  5.]],\n",
      "\n",
      "        [[ 6.,  7.,  8.]],\n",
      "\n",
      "        [[ 9., 10., 11.]]])\n",
      "torch.Size([4, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "# 3차원 텐서 크기만 변경(샘플 개수)\n",
    "print(ft.view([-1, 1, 3]))\n",
    "print(ft.view([-1, 1, 3]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5) 스퀴즈 - 1인 차원을 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "# 5) 스퀴즈 - 1인 차원을 제거함\n",
    "ft = torch.FloatTensor([[0], [1], [2]])  # 2차원\n",
    "print(ft)\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2.])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.squeeze())   # 1인 차원이 제거되어 1차원 벡터로 변환됨\n",
    "print(ft.squeeze().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6) 언스퀴즈 - 특정 위치에 1인 차원을 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# 특정 위치에 1인 차원을 추가함\n",
    "ft = torch.Tensor([0, 1, 2])  # 1차원\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.]])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.unsqueeze(0)) # 인덱스가 0부터 시작하므로 0은 첫번째 차원을 의미\n",
    "print(ft.unsqueeze(0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.]])\n",
      "torch.Size([1, 3])\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "# 동일한 처리를 view로\n",
    "print(ft.view(1, -1))  # 첫번째 차원을 1로 (x축(행)이 1개), 두번째 차원은 임의\n",
    "print(ft.view(1, -1).shape)\n",
    "print(ft.view(3, -1))  # 첫번째 차원을 3으로 (x축(행)이 3개), 두번째 차원은 임의\n",
    "print(ft.view(3, -1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n",
      "torch.Size([3, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.],\n",
       "         [1.],\n",
       "         [2.]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question\n",
    "ft = torch.FloatTensor([[0, 1, 2]])  # 2차원\n",
    "ft2 = torch.FloatTensor([[0], [1], [2]])  # 2차원\n",
    "print(ft.shape)\n",
    "print(ft2.shape)\n",
    "\n",
    "# 1) ft.unsqueeze(1)\n",
    "#ft.unsqueeze(0).shape\n",
    "\n",
    "# 2) ft.unsquessze(-1)\n",
    "\n",
    "# 3) view\n",
    "ft2.view(1, 3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.],\n",
       "         [1.],\n",
       "         [2.]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.view(1, 3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft3 = torch.FloatTensor([[[0, 1, 2]]])\n",
    "ft3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make 1, 3, 2\n",
    "torch.FloatTensor([[[2, 1],[3, 4],[4, 3]]]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (개념) 단어의 표현 방법 https://wikidocs.net/60852\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원핫 인코딩 한계\n",
    " 1. 단어 개수가 늘어날수록 벡터 공간도 함께 늘어남(저장 공간 측면에서 매우 비효율적)\n",
    " 2. 단어의 유사도를 표현하지 못함\n",
    "\n",
    "### 희소 표현(희소 벡터)\n",
    " - 벡터 또는 행렬(matrix)의 값이 대부분 0으로 표현되는 방법을 희소 표현(sparse representation)이라고 함). 원-핫 벡터는 희소 벡터이다(원핫 인코딩의 한계를 그대로 가짐)\n",
    " \n",
    " Ex) 강아지 = [ 0 0 0 0 1 0 0 0 0 0 0 0 ... 중략 ... 0] # 이 때 1 뒤의 0의 수는 9995개. 차원은 10,000 (단어가 10,000개라고 가정 시)\n",
    " \n",
    "### 밀집 표현(밀집 벡터)\n",
    " - 사용자가 밀집 표현의 차원을 128로 설정한다면, 모든 단어의 벡터 표현의 차원은 128로 바뀌면서 모든 값이 실수가 된다.\n",
    " \n",
    " Ex) 강아지 = [0.2 1.8 1.1 -2.1 1.1 2.8 ... 중략 ...] # 이 벡터의 차원은 128\n",
    " \n",
    "### 워드 임베딩\n",
    " - 단어를 밀집 벡터(dense vector)의 형태로 표현하는 방법을 워드 임베딩이라고 함. 그리고 이 밀집 벡터를 워드임베딩 과정을 통해 나온 결과라고 하여 임베딩 벡터라고도 한다.\n",
    " \n",
    " - 워드 임베딩 방법론으로는 LSA, Word2Vec, FastText, Glove 등이 있음 \n",
    " - Pytorch 에서 제공하는 도구인 nn.embedding()는 앞서 언급한 방법들을 사용하지는 않지만, 단어를 랜덤한 값을 가지는 밀집 벡터로 변환한 뒤에, 인공 신경망의 가중치를 학습하는 것과 같은 방식으로 단어 벡터를 학습하는 방법을 사용함\n",
    " \n",
    "### 워드투벡터(Word2Vec)\n",
    " - 단어 간 유사도를 반영할 수 있도록 단어의 의미를 벡터화 하는 방법\n",
    " - 분산 표현 : 단어의 '의미'를 다차원 공간에 벡터화하는 방법\n",
    " - 이렇게 분산 표현을 이용하여 단어의 유사도를 벡터화하는 작업은 워드 임베딩(embedding) 작업에 속하기 때문에 이렇게 표현된 벡터 또한 임베딩 벡터(embedding vector)라고 하며, 저차원을 가지므로 바로 앞의 챕터에서 배운 밀집 벡터(dense vector)에도 속함\n",
    " \n",
    "### 분산표현\n",
    " - 요약하면 희소 표현이 고차원에 각 차원이 분리된 표현 방법이었다면, 분산 표현은 저차원에 단어의 의미를 여러 차원에다가 분산하여 표현한다. 이런 표현 방법을 사용하면 단어 간 유사도를 계산할 수 있다.\n",
    "\n",
    " - 이를 위한 학습 방법으로는 NNLM, RNNLM 등이 있으나 요즘에는 해당 방법들의 속도를 대폭 개선시킨 Word2Vec가 많이 쓰이고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch의 nn.Embedding()\n",
    "- 파이토치에서는 임베딩 벡터를 사용하는 방법이 크게 두 가지가 있다. \n",
    "    - 임베딩 층(embedding layer)을 만들어 훈련 데이터로부터 처음부터 임베딩 벡터를 학습하는 방법\n",
    "    - 미리 사전에 훈련된 임베딩 벡터(pre-trained word embedding)들을 가져와 사용하는 방법\n",
    "    \n",
    "### 임베딩 층은 룩업 테이블이다.\n",
    "- 임베딩 층의 입력으로 사용하기 위해서 입력 시퀀스의 각 단어들은 모두 정수 인코딩이 되어있어야 한다\n",
    "\n",
    "어떤 단어 → 단어에 부여된 고유한 정수값 → 임베딩 층 통과 → 밀집 벡터\n",
    "\n",
    "1) 임베딩 층은 입력 정수에 대해 밀집 벡터(dense vector)로 매핑\n",
    "\n",
    "2) 이 밀집 벡터는 인공 신경망의 학습 과정에서 가중치가 학습되는 것과 같은 방식으로 훈련됨. \n",
    "\n",
    "3) 훈련 과정에서 단어는 모델이 풀고자하는 작업에 맞는 값으로 업데이트 된다. \n",
    "\n",
    "이 밀집 벡터를 임베딩 벡터라고 부른다.\n",
    "    \n",
    "\n",
    "정수를 밀집 벡터 또는 임베딩 벡터로 맵핑한다는 것은 어떤 의미일까요? 특정 단어와 맵핑되는 정수를 인덱스로 가지는 테이블로부터 임베딩 벡터 값을 가져오는 룩업 테이블이라고 볼 수 있습니다. 그리고 이 테이블은 단어 집합의 크기만큼의 행을 가지므로 모든 단어는 고유한 임베딩 벡터를 가집니다.\n",
    "\n",
    "룩업테이블에 있는 단어의 임베딩 벡터들은 역전파 과정에서 값이 학습된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'to': 2, 'code': 3, 'need': 4, 'know': 5, 'you': 6, 'how': 7, '<unk>': 0, '<pad>': 1}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "train_data = 'you need to know how to code'\n",
    "word_set = set(train_data.split()) # 중복을 제거한 단어들의 집합인 단어 집합 생성.\n",
    "vocab = {word: i+2 for i, word in enumerate(word_set)}  # 단어 집합의 각 단어에 고유한 정수 매핑.\n",
    "vocab['<unk>'] = 0\n",
    "vocab['<pad>'] = 1\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 단어 집합의 크기만큼의 행을 가지는 테이블 생성. (임베딩 벡터의 차원은 3이라 가정)\n",
    "embedding_table = torch.FloatTensor([\n",
    "                               [ 0.0,  0.0,  0.0],  # 0\n",
    "                               [ 0.0,  0.0,  0.0],  # 1\n",
    "                               [ 0.2,  0.9,  0.3],  # 2\n",
    "                               [ 0.1,  0.5,  0.7],  # 3\n",
    "                               [ 0.2,  0.1,  0.8],  # 4\n",
    "                               [ 0.4,  0.1,  0.1],  # 5\n",
    "                               [ 0.1,  0.8,  0.9],  # 6\n",
    "                               [ 0.6,  0.1,  0.1]])  # 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1000, 0.8000, 0.9000],\n",
      "        [0.2000, 0.1000, 0.8000],\n",
      "        [0.2000, 0.9000, 0.3000],\n",
      "        [0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# 임의의 샘플 문장\n",
    "sample = 'you need to run'.split()\n",
    "idxes=[]\n",
    "# 각 단어를 정수로 변환\n",
    "for word in sample:\n",
    "  try:\n",
    "    idxes.append(vocab[word])\n",
    "  except KeyError: # 단어 집합에 없는 단어일 경우 <unk>로 대체된다.\n",
    "    idxes.append(vocab['<unk>'])\n",
    "idxes = torch.LongTensor(idxes) # tensor([6, 4, 2, 0])\n",
    "\n",
    "# 룩업 테이블\n",
    "lookup_result = embedding_table[idxes, :] # 각 정수를 인덱스로 임베딩 테이블에서 값을 가져온다.\n",
    "print(lookup_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 임베딩 층 사용하기\n",
    "\n",
    "nn.Embedding은 크게 두 가지 인자를 받는데 각각 num_embeddings과 embedding_dim입니다.\n",
    "\n",
    " - num_embeddings : 임베딩을 할 단어들의 개수. 다시 말해 단어 집합의 크기.\n",
    " - embedding_dim : 임베딩 할 벡터의 차원. 사용자가 정해주는 하이퍼파라미터이다.\n",
    " - padding_idx : 선택적으로 사용하는 인자. 패딩을 위한 토큰의 인덱스를 알려준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = 'you need to know how to code'\n",
    "word_set = set(train_data.split()) # 중복을 제거한 단어들의 집합인 단어 집합 생성.\n",
    "vocab = {tkn: i+2 for i, tkn in enumerate(word_set)}  # 단어 집합의 각 단어에 고유한 정수 맵핑.\n",
    "vocab['<unk>'] = 0\n",
    "vocab['<pad>'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "embedding_layer = nn.Embedding(num_embeddings = len(vocab), \n",
    "                               embedding_dim = 3,\n",
    "                               padding_idx = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 2.8888,  1.5721,  0.5345],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.2195,  1.0877, -0.9658],\n",
      "        [-0.9405,  0.2130,  1.4181],\n",
      "        [ 0.8394,  0.2810,  0.8668],\n",
      "        [ 0.2448, -0.5023,  0.9176],\n",
      "        [-0.8760,  0.8622,  0.7863],\n",
      "        [-0.4443,  0.6083, -0.7428]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#print(embedding_layer)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사전 훈련된 워드 임베딩(Pretrained Word Embedding) : https://wikidocs.net/64904\n",
    " - nn.Embedding()을 사용하는 것보다 다른 텍스트 데이터로 사전 훈련되어 있는 임베딩 벡터를 불러오는 것이 나은 선택일 수 있다.\n",
    " - 훈련 데이터가 적다면 파이토치의 nn.Embedding()으로 해당 문제에 충분히 특화된 임베딩 벡터를 만들어내는 것이 쉽지 않다. 이 경우, 해당 문제에 특화된 것은 아니지만 보다 일반적이고 보다 많은 훈련 데이터로 이미 Word2Vec이나 GloVe 등으로 학습되어져 있는 임베딩 벡터들을 사용하는 것이 성능의 개선을 가져올 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-f8dab9081d39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "from torchtext import data, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (구현부) 워드 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_batch(sentences):\n",
    "    input_batch = [[src_vocab[n] for n in sentences[0].split()]] # input 문장을 단어로 분리 후 워드 임베딩\n",
    "    output_batch = [[tgt_vocab[n] for n in sentences[1].split()]] # output 문장을 단어로 분리 후 워드 임베딩\n",
    "    target_batch = [[tgt_vocab[n] for n in sentences[2].split()]] \n",
    "    return torch.LongTensor(input_batch), torch.LongTensor(output_batch), torch.LongTensor(target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "7\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "src_vocab = {'P': 0, 'ich': 1, 'mochte': 2, 'ein': 3, 'bier': 4}\n",
    "src_vocab_size = len(src_vocab)\n",
    "print(src_vocab_size)\n",
    "tgt_vocab = {'P': 0, 'i': 1, 'want': 2, 'a': 3, 'beer': 4, 'S': 5, 'E': 6}\n",
    "number_dict = {i: w for i, w in enumerate(tgt_vocab)}\n",
    "tgt_vocab_size = len(tgt_vocab)\n",
    "print(tgt_vocab_size)\n",
    "d_model = 512\n",
    "print(d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(5, 512)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab_size = 5 # 단어 사전 크기\n",
    "d_model = 512  # 단어 임베딩 크기 지정(하이퍼 파라미터)\n",
    "src_len = 5 # length of source?\n",
    "n_layers = 6  # number of Encoder of Decoder Layer\n",
    "n_heads = 8  # number of heads in Multi-Head Attention\n",
    "\n",
    "nn.Embedding(src_vocab_size, d_model) # 128 크기의 단어수 만큼 임베딩 벡터 공간을 만듬\n",
    "# nn.Embedding.from_pretrained(get_sinusoid_encoding_table(src_len+1, d_model),freeze=True) # Positional Encoding\n",
    "# nn.ModuleList([EncoderLayer() for _ in range(n_layers)]) # 인코더 레이어 생성. ModuleList를 활용하여 레이어 층만큼 반복 생성. 총 6개 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=512, bias=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 512  # 단어 임베딩 크기 지정(하이퍼 파라미터)\n",
    "n_heads = 8  # number of heads in Multi-Head Attention\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "\n",
    "W_Q = nn.Linear(d_model, d_k * n_heads)  # input : 512 , output : 64 * 8(=512)\n",
    "W_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P': 0, 'bier': 4, 'ein': 3, 'ich': 1, 'mochte': 2}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab = {'P': 0, 'ich': 1, 'mochte': 2, 'ein': 3, 'bier': 4}\n",
    "src_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'E': 6, 'P': 0, 'S': 5, 'a': 3, 'beer': 4, 'i': 1, 'want': 2}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_vocab = {'P': 0, 'i': 1, 'want': 2, 'a': 3, 'beer': 4, 'S': 5, 'E': 6}\n",
    "tgt_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'P', 1: 'i', 2: 'want', 3: 'a', 4: 'beer', 5: 'S', 6: 'E'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_dict = {i: w for i, w in enumerate(tgt_vocab)}\n",
    "number_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_batch = [[src_vocab[n] for n in sentences[0].split()]]\n",
    "output_batch = [[tgt_vocab[n] for n in sentences[1].split()]]\n",
    "target_batch = [[tgt_vocab[n] for n in sentences[2].split()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4, 0]]\n",
      "[[5, 1, 2, 3, 4]]\n",
      "[[1, 2, 3, 4, 6]]\n"
     ]
    }
   ],
   "source": [
    "print(input_batch)\n",
    "print(output_batch)\n",
    "print(target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4, 0]])\n",
      "tensor([[5, 1, 2, 3, 4]])\n",
      "tensor([[1, 2, 3, 4, 6]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.LongTensor(input_batch))\n",
    "print(torch.LongTensor(output_batch))\n",
    "print(torch.LongTensor(target_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(5, 512)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 워드임베딩(각 단어 별 512 차원 할당)\n",
    "src_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "src_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 512])\n",
      "tensor([[[-0.4063,  1.4891,  0.1087,  ...,  0.1397,  0.4253, -0.3399],\n",
      "         [-0.6871,  1.1994,  0.9692,  ..., -0.1375, -0.6566, -1.9956],\n",
      "         [ 1.0790,  0.6000, -0.7118,  ..., -1.5002,  0.5742,  1.2614],\n",
      "         [ 0.7972,  1.0337,  0.5715,  ..., -1.2297, -0.2310, -0.6980],\n",
      "         [ 0.0772,  1.0031,  1.0915,  ..., -0.3470,  1.2601, -1.4068]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "enc_inputs, dec_inputs, target_batch = make_batch(sentences)\n",
    "\n",
    "enc_outputs = src_emb(enc_inputs)  # 5개의 단어 512차원의 워드임베딩 확인\n",
    "print(enc_outputs.shape)\n",
    "print(enc_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (핸들링) Multihead Attention 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128, 768])\n",
      "torch.Size([32, 128, 12, 64])\n",
      "torch.Size([32, 128, 12, 64])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.rand([32, 128, 768])\n",
    "print(t1.shape)\n",
    "\n",
    "new_shape = t1.size()[:-1] + (12, 64)\n",
    "print(new_shape)\n",
    "\n",
    "q = t1.view(new_shape)\n",
    "print(q.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (개념) Multihead\n",
    "https://catsirup.github.io/ai/2020/04/09/transformer-code.html  \n",
    "http://incredible.ai/nlp/2020/02/29/Transformer/#261-q-k-and-v\n",
    "\n",
    "예를 들어서 Q (256, 33, 512), K (256, 33, 512), V (256, 33, 512) 이렇게 embedding vectors가 있을때,\n",
    "33은 33개의 단어가 있다는 뜻이고, 단어 하나당 512개의 dense vector로 표현이 된다.\n",
    "이때 512에 해당되는 부분에 대해서, single attention을 하는 것이 아니라, 512이 부분을 \n",
    "h개로 쪼개서 multi-head attention을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "torch.Size([256, 33, 512])\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "n_head = 8\n",
    "d_k = d_model//n_head\n",
    "d_v = d_model//n_head\n",
    "print(d_k)\n",
    "\n",
    "Q = torch.rand(256, 33, d_model) # Encoder시에는 Q, K, V는 모두 동일하다\n",
    "K = Q\n",
    "V = Q\n",
    "\n",
    "linear_q = nn.Linear(d_model, n_head * d_k, bias=False) # 512 * 512 (input * output)\n",
    "linear_k = nn.Linear(d_model, n_head * d_k, bias=False) # 512 * 512 (input * output)\n",
    "linear_v = nn.Linear(d_model, n_head * d_k, bias=False) # 512 * 512 (input * output)\n",
    "\n",
    "q = linear_q(Q) # Linear Transformation (256, 33, 512)\n",
    "print(q.shape)\n",
    "k = linear_k(Q) # Linear Transformation (256, 33, 512)\n",
    "v = linear_v(Q) # Linear Transformation (256, 33, 512)\n",
    "\n",
    "# Multi Head : d_model(512) vector부분을 h개 (8개)로 나눈다\n",
    "q_s = q.view(256, 33, n_head * d_k) # (256, 33, 8, 64)\n",
    "k_s = k.view(256, 33, n_head * d_k) # (256, 33, 8, 64)\n",
    "v_s = v.view(256, 33, n_head * d_v) # (256, 33, 8, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 16])\n",
      "torch.Size([3, 32, 16])\n",
      "torch.Size([32, 16, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(16, 32, 3)\n",
    "y = x.transpose(0, 2)\n",
    "print(y.shape)\n",
    "z = x.permute(2, 1, 0)\n",
    "print(z.shape)\n",
    "\n",
    "p = x.transpose(-2, 0)\n",
    "print(p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (모듈) MultiHead Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: torch.Size([256, 33, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    512인 embedding vector를 -> n_head에 따라서 나눈다.\n",
    "    예를 들어, n_head=8일 경우 512 vector를 -> 8 * 64 vector 로 변형한다\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, n_head: int, dropout: float = 0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_head = n_head\n",
    "        self.dk = embed_dim // n_head\n",
    "        self.dv = embed_dim // n_head\n",
    "\n",
    "        self.linear_q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.linear_v = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.linear_k = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.linear_f = nn.Linear(embed_dim, embed_dim, bias=False)  # Final linear layer\n",
    "\n",
    "        self.attention = ScaleDotProductAttention(self.dk, dropout=dropout)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "         * 마지막 skip connection 은 layer 부분에서 구현함\n",
    "        \"\"\"\n",
    "        batch_size, n_head, dk, dv = q.size(0), self.n_head, self.dk, self.dv\n",
    "\n",
    "        # Linear Transformation (256, 33, 512)\n",
    "        # Multi Head : d_model(512) vector부분을 h개 (8개) 로 나눈다\n",
    "        q = self.linear_q(q).view(batch_size, -1, n_head, dk)\n",
    "        k = self.linear_k(k).view(batch_size, -1, n_head, dk)\n",
    "        v = self.linear_v(v).view(batch_size, -1, n_head, dv)\n",
    "\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        scores = self.attention(q, k, v, mask)\n",
    "\n",
    "        # multi head dimension 을 원래의 형태로 되돌린다\n",
    "        # (batch, n_head, seq_len, d_v) (256, 8, 33, 64) --> (batch, seq_len, n_head, d_v) (256, 33, 8, 64)\n",
    "        scores = scores.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n",
    "\n",
    "        # Final linear Layer\n",
    "        scores = self.linear_f(scores)\n",
    "        return scores\n",
    "\n",
    "    \n",
    "class ScaleDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention(Q, K, V) = softmax( (QK^T)/sqrt(d_k) )\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_k: int, dropout: float):\n",
    "        \"\"\"\n",
    "        :param d_k: the number of heads\n",
    "        \"\"\"\n",
    "        super(ScaleDotProductAttention, self).__init__()\n",
    "        self.sqrt_dk = d_k ** 0.5  # 8 = 64**0.5\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param q: Queries (256 batch, 8 d_k, 33 sequence, 64)\n",
    "        :param k: Keys    (256, 8, 33, 64)\n",
    "        :param v: Values  (256, 8, 33, 64)\n",
    "        :param mask: mask (256, 1, 28) Source Mask\n",
    "        :return: scaled dot attention: (256, 8, 33, 64)\n",
    "        \"\"\"\n",
    "        attn = (q @ k.transpose(-2, -1)) / self.sqrt_dk\n",
    "        if mask is not None:  # 논문에는 안나온 내용. 하지만 masking을 해주어야 한다\n",
    "            mask = mask.unsqueeze(1)\n",
    "            attn = attn.masked_fill(~mask, -1e9) # -10000000 같은 큰 음수값을 할당. (softmax에서 0으로 나오게 됨)\n",
    "\n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))  # softmax 이후 dropout도 논문에는 없으나 해야 한다\n",
    "        output = attn @ v  # (256, 8, 33, 64)\n",
    "        return output\n",
    "\n",
    "\n",
    "input_tensor = torch.rand(256, 33, 512)\n",
    "\n",
    "attention = MultiHeadAttention(512, 8)\n",
    "output = attention(input_tensor, input_tensor, input_tensor, None)\n",
    "\n",
    "print('output:', output.shape)  # output: torch.Size([256, 33, 512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (모듈) Masking Function\n",
    "\n",
    "- Padding Mask: input sentence안에 padding이 있을경우 masking은 attention outputs을 zero out 시킨다.\n",
    "- Look Ahead Mask: Decoder가 다음 단어를 예측할때, 그 다음 단어 및 뒤에 나오는 문장을 미리 peaking ahead하지 않도록 막는다. (A.K.A No Peaking Mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tuple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-bedd2ad544b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m                 \u001b[0mtrg\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                 \u001b[0msrc_pad_idx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                 trg_pad_idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0msrc_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_create_padding_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_pad_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtrg_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Tuple' is not defined"
     ]
    }
   ],
   "source": [
    "def create_mask(src: torch.Tensor,\n",
    "                trg: torch.Tensor,\n",
    "                src_pad_idx: int,\n",
    "                trg_pad_idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    src_mask = _create_padding_mask(src, src_pad_idx)\n",
    "    trg_mask = None\n",
    "    if trg is not None:\n",
    "        trg_mask = _create_padding_mask(trg, trg_pad_idx)  # (256, 1, 33)\n",
    "        nopeak_mask = _create_nopeak_mask(trg)  # (1, 33, 33)\n",
    "        trg_mask = trg_mask & nopeak_mask  # (256, 33, 33)\n",
    "    return src_mask, trg_mask\n",
    "\n",
    "def _create_padding_mask(seq: torch.Tensor, pad_idx: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    seq 형태를  (256, 33) -> (256, 1, 31) 이렇게 변경합니다.\n",
    "\n",
    "    아래와 같이 padding index부분을 False로 변경합니다. (리턴 tensor)\n",
    "    아래의 vector 하나당 sentence라고 보면 되고, True로 되어 있는건 단어가 있다는 뜻.\n",
    "    tensor([[[ True,  True,  True,  True, False, False, False]],\n",
    "            [[ True,  True, False, False, False, False, False]],\n",
    "            [[ True,  True,  True,  True,  True,  True, False]]])\n",
    "    \"\"\"\n",
    "    return (seq != pad_idx).unsqueeze(-2)\n",
    "\n",
    "def _create_nopeak_mask(trg) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    NO PEAK MASK\n",
    "    Target의 경우 그 다음 단어를 못보게 가린다\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = trg.size()\n",
    "    nopeak_mask = (1 - torch.triu(torch.ones(1, seq_len, seq_len, device=trg.device), diagonal=1)).bool()\n",
    "    return nopeak_mask\n",
    "\n",
    "sentences = torch.Tensor([[2., 2., 2., 2., 1., 1., 1.],\n",
    "                          [2., 2., 1., 1., 1., 1., 1.],\n",
    "                          [2., 2., 2., 2., 2., 2., 1.]])\n",
    "src_mask, trg_mask = create_mask(sentences, sentences, 1, 1)\n",
    "\n",
    "print('sentences:', sentences.shape)\n",
    "print('src_mask :', src_mask.shape)\n",
    "print('trg_mask :', trg_mask.shape)\n",
    "# sentences: torch.Size([3, 7])\n",
    "# src_mask : torch.Size([3, 1, 7])\n",
    "# trg_mask : torch.Size([3, 7, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (모듈) Masking and Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.5000, 2.9000, 1.0000, 1.0000])\n",
      "tensor([ 3.5000e+00,  2.9000e+00, -1.0000e+09, -1.0000e+09])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ghdls\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6456563, 0.3543437, 0.0, 0.0]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "mask = torch.BoolTensor([True, True, False, False])\n",
    "vector = torch.FloatTensor([3.5, 2.9, 1, 1])\n",
    "masked_vector = vector.masked_fill(~mask, -1e9) # False에만 마스크 음수 값 적용\n",
    "\n",
    "#list(F.softmax(masked_vector)).numpy()\n",
    "print(vector)\n",
    "print(masked_vector)\n",
    "\n",
    "list(F.softmax(masked_vector).numpy())  # 0~1사이로 변환하고 총 합은 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (구현부) Multihead Attention - 이어서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4063,  1.4891,  0.1087,  ...,  0.1397,  0.4253, -0.3399],\n",
      "         [-0.6871,  1.1994,  0.9692,  ..., -0.1375, -0.6566, -1.9956],\n",
      "         [ 1.0790,  0.6000, -0.7118,  ..., -1.5002,  0.5742,  1.2614],\n",
      "         [ 0.7972,  1.0337,  0.5715,  ..., -1.2297, -0.2310, -0.6980],\n",
      "         [ 0.0772,  1.0031,  1.0915,  ..., -0.3470,  1.2601, -1.4068]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "Q = K = V = enc_outputs   # 위에서 실행하고 와야함\n",
    "residual, batch_size = Q, Q.size(0)\n",
    "print(residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_Q = nn.Linear(d_model, d_k * n_heads) # Q Linear Transformation, input : 512 , output : 64 * 8(=512)\n",
    "W_K = nn.Linear(d_model, d_k * n_heads) # K Linear Transformation\n",
    "W_V = nn.Linear(d_model, d_v * n_heads) # V Linear Transformation\n",
    "linear = nn.Linear(n_heads * d_v, d_model) # 전이 층\n",
    "layer_norm = nn.LayerNorm(d_model) # Normalize 층 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_s = W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "k_s = W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "v_s = W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# padding mask\n",
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4, 0]])\n",
      "tensor([[[False, False, False, False,  True],\n",
      "         [False, False, False, False,  True],\n",
      "         [False, False, False, False,  True],\n",
      "         [False, False, False, False,  True],\n",
      "         [False, False, False, False,  True]]])\n"
     ]
    }
   ],
   "source": [
    "# Mask\n",
    "print(enc_inputs)  # 'ich mochte ein bier P'\n",
    "enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)   \n",
    "print(enc_self_attn_mask) # 최초 마스크\n",
    "\n",
    "#attn_mask = enc_self_attn_mask\n",
    "#attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # Multi Head용 마스크로 차원 확대 후, head 수 만큼 마스크 복제\n",
    "#print(attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "tensor([[1, 2, 3, 4, 0]])\n",
      "tensor([[False, False, False, False,  True]])\n",
      "tensor([[[False, False, False, False,  True]]])\n"
     ]
    }
   ],
   "source": [
    "# 마스크 쪼개서 확인해보기\n",
    "seq_q = seq_k = enc_inputs\n",
    "batch_size , len_q = enc_inputs.size()\n",
    "batch_size , len_k = enc_inputs.size()\n",
    "print(len_q)\n",
    "print(seq_k.data)\n",
    "print(seq_k.data.eq(0)) # 패딩 값은 0이므로 true로 바꿈 # https://pytorch.org/docs/stable/generated/torch.eq.html\n",
    "print(seq_k.data.eq(0).unsqueeze(1)) # 차원 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (구현부) ScaleDotProductAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "Q = q_s\n",
    "K = k_s\n",
    "V = v_s\n",
    "\n",
    "scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "#print(scores)\n",
    "#scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "attn = nn.Softmax(dim=-1)(scores) # 스코어값을 0~1 양수로 만듬, 얼마나 각 단어들의 표현이 들어갈지 결정?\n",
    "context = torch.matmul(attn, V) # 가장 관련있는 단어만 필터링하는 역할\n",
    "#print(scores)\n",
    "print(attn.shape)\n",
    "#print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 8, 64])\n",
      "torch.Size([1, 5, 512])\n",
      "tensor([[[ 0.2403, -0.1162, -0.5623,  ...,  0.2304, -0.0953,  0.2887],\n",
      "         [ 0.2393, -0.1613, -0.5267,  ...,  0.3559,  0.0550,  0.3904],\n",
      "         [ 0.1502, -0.1162, -0.4103,  ...,  0.3567,  0.0514,  0.3648],\n",
      "         [ 0.0307, -0.1683, -0.4966,  ...,  0.3932,  0.0814,  0.2750],\n",
      "         [-0.0041, -0.3412, -0.5383,  ...,  0.3088, -0.0053,  0.2674]]],\n",
      "       grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "n_heads = 8\n",
    "print(context.transpose(1, 2).shape)\n",
    "print(context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v).shape)\n",
    "print(context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v))\n",
    "\n",
    "final_context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0592,  0.3010,  0.1240,  ..., -0.2185, -0.0648, -0.0167],\n",
      "         [-0.0122,  0.1712,  0.0819,  ..., -0.1930, -0.0732, -0.0295],\n",
      "         [-0.1194,  0.0916,  0.0926,  ..., -0.1933, -0.0085, -0.0780],\n",
      "         [-0.0061,  0.2158,  0.1566,  ..., -0.1817, -0.0698, -0.0209],\n",
      "         [-0.1089,  0.2615,  0.1085,  ..., -0.2383, -0.1395, -0.0796]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "linear = nn.Linear(n_heads * d_v, d_model) # 8 * 64 , 512\n",
    "print(linear(final_context))\n",
    "output = linear(final_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (구현부) Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4599,  1.1915,  0.4021,  ...,  0.1777, -1.0030,  0.7643],\n",
      "         [-0.3568,  0.1438, -0.7795,  ..., -0.3013, -0.0252, -0.6272],\n",
      "         [ 1.6117,  0.2404, -0.6821,  ..., -1.9615,  0.9758,  0.5678],\n",
      "         [-0.1346,  0.2992,  0.2553,  ..., -0.4579, -1.5068,  1.2774],\n",
      "         [-0.0033, -0.8893, -0.7915,  ..., -0.7058, -0.3482, -1.2012]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "torch.Size([1, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "layer_norm = nn.LayerNorm(d_model)\n",
    "print(layer_norm(output + residual))  # 층이 깊어질수록 소실되므로 residual값을 매번 더해줌\n",
    "final_output = layer_norm(output + residual)\n",
    "print(final_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (구현부) Position-wise Feed-Forward Network(FFN)\n",
    " - Down Sampling목적? X \n",
    " - 추상화 목적임. 더 미세한 변화를 확인하기 위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "d_ff = 2048\n",
    "conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "layer_norm = nn.LayerNorm(d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.4633, 0.7812, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1064, 0.0000, 0.1421, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0754, 0.3704, 0.2103, 0.0000, 0.0000],\n",
       "         [0.6813, 0.0000, 0.0000, 0.2092, 0.0000],\n",
       "         [0.3857, 1.2783, 0.8413, 0.0000, 0.6361]]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residual = final_output\n",
    "conv1(final_output.transpose(1,2))\n",
    "#conv1(final_output.transpose(1,2)).shape\n",
    "nn.ReLU()(conv1(final_output.transpose(1,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (구현부) 디코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "src_vocab = {'P': 0, 'ich': 1, 'mochte': 2, 'ein': 3, 'bier': 4}\n",
    "src_vocab_size = len(src_vocab)\n",
    "\n",
    "d_model = 512\n",
    "tgt_vocab = {'P': 0, 'i': 1, 'want': 2, 'a': 3, 'beer': 4, 'S': 5, 'E': 6}\n",
    "number_dict = {i: w for i, w in enumerate(tgt_vocab)}\n",
    "tgt_vocab_size = len(tgt_vocab)\n",
    "print(tgt_vocab_size)\n",
    "#src_len = 5 # length of source\n",
    "#tgt_len = 5 # length of target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_batch(sentences):\n",
    "    input_batch = [[src_vocab[n] for n in sentences[0].split()]] # input 문장을 단어로 분리 후 워드 임베딩\n",
    "    output_batch = [[tgt_vocab[n] for n in sentences[1].split()]] # output 문장을 단어로 분리 후 워드 임베딩\n",
    "    target_batch = [[tgt_vocab[n] for n in sentences[2].split()]] \n",
    "    return torch.LongTensor(input_batch), torch.LongTensor(output_batch), torch.LongTensor(target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " enc_inputs, dec_inputs, target_batch = make_batch(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 워드 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.4927,  1.6065,  0.2568,  ..., -0.8925, -0.5202,  0.6003],\n",
      "         [ 1.5593,  0.8828, -1.4681,  ..., -0.0040, -1.1307,  0.0302],\n",
      "         [-1.3885,  0.0118,  1.1959,  ...,  1.2113,  0.3704, -0.1339],\n",
      "         [ 0.7743, -1.6328, -1.5545,  ...,  1.4239, -1.1042, -0.3252],\n",
      "         [-0.6339, -1.1806,  0.2463,  ...,  0.0412, -0.9065,  0.1756]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "# dec_inputs(main에서 전달), enc_inputs(main에서 전달), enc_outputs(인코더 결과)\n",
    "sentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E']\n",
    "enc_inputs, dec_inputs, target_batch = make_batch(sentences)\n",
    "\n",
    "# Embedding Layer\n",
    "tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "# Word Embedding\n",
    "dec_outputs = tgt_emb(dec_inputs)\n",
    "print(dec_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look-ahead mask\n",
    "def get_attn_subsequent_mask(seq):\n",
    "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1)\n",
    "    subsequent_mask = torch.from_numpy(subsequent_mask).byte()\n",
    "    return subsequent_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# padding mask\n",
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 마스크 적용(패딩, 룩-어헤드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더부 마스크(패딩 + 룩-어헤드)\n",
    "dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs) # 패딩마스크 \n",
    "dec_self_attn_subsequent_mask = get_attn_subsequent_mask(dec_inputs) # 룩어헤드 마스크\n",
    "dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequent_mask), 0) # 더한값이 0보다 클 경우 true표기. # https://pytorch.org/docs/stable/generated/torch.gt.html\n",
    "\n",
    "# 인코더부 패딩마스크\n",
    "dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 디코더 레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MultiHeadAttention' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-eae36d571dcf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdec_self_attn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 첫번째 서브층\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdec_enc_attn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 두번째 서브층\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpos_ffn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPoswiseFeedForwardNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 세번째 서브층\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MultiHeadAttention' is not defined"
     ]
    }
   ],
   "source": [
    "dec_self_attn = MultiHeadAttention() # 첫번째 서브층 (셀프 어텐션. 인코더와 동일)\n",
    "dec_enc_attn = MultiHeadAttention() # 두번째 서브층 (인코더-디코더 어텐션)\n",
    "pos_ffn = PoswiseFeedForwardNet() # 세번째 서브층\n",
    "\n",
    "dec_outputs, dec_self_attn = dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask) # 인코더부와 형태 동일\n",
    "#dec_outputs, dec_enc_attn = dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask) #인코더와 디코더 데이터 사용\n",
    "#dec_outputs = pos_ffn(dec_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
