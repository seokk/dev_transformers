{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [참조](https://tutorials.pytorch.kr/beginner/pytorch_with_examples.html)\n",
    "## [해석참고](https://pbj0812.tistory.com/137)\n",
    " - 머리속으로 그려보며 가공되는 텐서들의 그림을 그려보며 1cycle을 짤 수 있어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy.dot(a,b) :  20\n"
     ]
    }
   ],
   "source": [
    "# Numpy 복습\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([2, 3, 4])\n",
    "\n",
    "print('numpy.dot(a,b) : ', np.dot(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n",
      " [[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n",
      "d\n",
      " [[0 1 2]\n",
      " [3 4 5]]\n",
      "numpy.dot(c, d)\n",
      "[[ 3  4  5]\n",
      " [ 9 14 19]\n",
      " [15 24 33]]\n",
      "\n",
      "numpy.dot(c, d).shape :\n",
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "c = np.arange(6).reshape(3, 2)\n",
    "d = np.arange(6).reshape(2, 3)\n",
    "\n",
    "print('c\\n', c)\n",
    "print('d\\n', d)\n",
    "print('numpy.dot(c, d)')\n",
    "print(np.dot(c, d))\n",
    "print()\n",
    "print('numpy.dot(c, d).shape :')\n",
    "print(np.dot(c, d).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.76327884, -0.47706742],\n",
       "       [ 1.07968177, -1.01061285],\n",
       "       [ 0.05184889, -0.02901645],\n",
       "       [ 1.07202986,  1.06132859],\n",
       "       [ 0.64864974,  1.01017646],\n",
       "       [-0.32026815,  2.27840162]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "d1 = np.random.rand(2, 6) # 0~1사이의 표준정규분포 난수를 matrix array(m, n) 생성\n",
    "w1 = np.random.randn(6, 2) # 평균0, 표준편차1의 가우시안 표준정규분포 난수를 matrix array(m,n) 생성\n",
    "\n",
    "w1\n",
    "#d1.dot(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.3959e-33, 1.3563e-19, 1.3563e-19],\n",
      "        [1.0899e+27, 4.5828e+30, 1.0302e+21],\n",
      "        [4.5630e+30, 8.9064e-15, 1.1578e+27],\n",
      "        [3.3876e-12, 7.6197e+31, 2.7490e+20],\n",
      "        [1.5769e-19, 7.5747e+23, 1.7754e+28]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "y = torch.LongTensor([1])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 32492087.973676823\n",
      "1 32492087.973676823\n",
      "2 32492087.973676823\n",
      "3 32492087.973676823\n",
      "4 32492087.973676823\n",
      "5 32492087.973676823\n",
      "6 32492087.973676823\n",
      "7 32492087.973676823\n",
      "8 32492087.973676823\n",
      "9 32492087.973676823\n",
      "10 32492087.973676823\n",
      "11 32492087.973676823\n",
      "12 32492087.973676823\n",
      "13 32492087.973676823\n",
      "14 32492087.973676823\n",
      "15 32492087.973676823\n",
      "16 32492087.973676823\n",
      "17 32492087.973676823\n",
      "18 32492087.973676823\n",
      "19 32492087.973676823\n",
      "20 32492087.973676823\n",
      "21 32492087.973676823\n",
      "22 32492087.973676823\n",
      "23 32492087.973676823\n",
      "24 32492087.973676823\n",
      "25 32492087.973676823\n",
      "26 32492087.973676823\n",
      "27 32492087.973676823\n",
      "28 32492087.973676823\n",
      "29 32492087.973676823\n",
      "30 32492087.973676823\n",
      "31 32492087.973676823\n",
      "32 32492087.973676823\n",
      "33 32492087.973676823\n",
      "34 32492087.973676823\n",
      "35 32492087.973676823\n",
      "36 32492087.973676823\n",
      "37 32492087.973676823\n",
      "38 32492087.973676823\n",
      "39 32492087.973676823\n",
      "40 32492087.973676823\n",
      "41 32492087.973676823\n",
      "42 32492087.973676823\n",
      "43 32492087.973676823\n",
      "44 32492087.973676823\n",
      "45 32492087.973676823\n",
      "46 32492087.973676823\n",
      "47 32492087.973676823\n",
      "48 32492087.973676823\n",
      "49 32492087.973676823\n",
      "50 32492087.973676823\n",
      "51 32492087.973676823\n",
      "52 32492087.973676823\n",
      "53 32492087.973676823\n",
      "54 32492087.973676823\n",
      "55 32492087.973676823\n",
      "56 32492087.973676823\n",
      "57 32492087.973676823\n",
      "58 32492087.973676823\n",
      "59 32492087.973676823\n",
      "60 32492087.973676823\n",
      "61 32492087.973676823\n",
      "62 32492087.973676823\n",
      "63 32492087.973676823\n",
      "64 32492087.973676823\n",
      "65 32492087.973676823\n",
      "66 32492087.973676823\n",
      "67 32492087.973676823\n",
      "68 32492087.973676823\n",
      "69 32492087.973676823\n",
      "70 32492087.973676823\n",
      "71 32492087.973676823\n",
      "72 32492087.973676823\n",
      "73 32492087.973676823\n",
      "74 32492087.973676823\n",
      "75 32492087.973676823\n",
      "76 32492087.973676823\n",
      "77 32492087.973676823\n",
      "78 32492087.973676823\n",
      "79 32492087.973676823\n",
      "80 32492087.973676823\n",
      "81 32492087.973676823\n",
      "82 32492087.973676823\n",
      "83 32492087.973676823\n",
      "84 32492087.973676823\n",
      "85 32492087.973676823\n",
      "86 32492087.973676823\n",
      "87 32492087.973676823\n",
      "88 32492087.973676823\n",
      "89 32492087.973676823\n",
      "90 32492087.973676823\n",
      "91 32492087.973676823\n",
      "92 32492087.973676823\n",
      "93 32492087.973676823\n",
      "94 32492087.973676823\n",
      "95 32492087.973676823\n",
      "96 32492087.973676823\n",
      "97 32492087.973676823\n",
      "98 32492087.973676823\n",
      "99 32492087.973676823\n",
      "100 32492087.973676823\n",
      "101 32492087.973676823\n",
      "102 32492087.973676823\n",
      "103 32492087.973676823\n",
      "104 32492087.973676823\n",
      "105 32492087.973676823\n",
      "106 32492087.973676823\n",
      "107 32492087.973676823\n",
      "108 32492087.973676823\n",
      "109 32492087.973676823\n",
      "110 32492087.973676823\n",
      "111 32492087.973676823\n",
      "112 32492087.973676823\n",
      "113 32492087.973676823\n",
      "114 32492087.973676823\n",
      "115 32492087.973676823\n",
      "116 32492087.973676823\n",
      "117 32492087.973676823\n",
      "118 32492087.973676823\n",
      "119 32492087.973676823\n",
      "120 32492087.973676823\n",
      "121 32492087.973676823\n",
      "122 32492087.973676823\n",
      "123 32492087.973676823\n",
      "124 32492087.973676823\n",
      "125 32492087.973676823\n",
      "126 32492087.973676823\n",
      "127 32492087.973676823\n",
      "128 32492087.973676823\n",
      "129 32492087.973676823\n",
      "130 32492087.973676823\n",
      "131 32492087.973676823\n",
      "132 32492087.973676823\n",
      "133 32492087.973676823\n",
      "134 32492087.973676823\n",
      "135 32492087.973676823\n",
      "136 32492087.973676823\n",
      "137 32492087.973676823\n",
      "138 32492087.973676823\n",
      "139 32492087.973676823\n",
      "140 32492087.973676823\n",
      "141 32492087.973676823\n",
      "142 32492087.973676823\n",
      "143 32492087.973676823\n",
      "144 32492087.973676823\n",
      "145 32492087.973676823\n",
      "146 32492087.973676823\n",
      "147 32492087.973676823\n",
      "148 32492087.973676823\n",
      "149 32492087.973676823\n",
      "150 32492087.973676823\n",
      "151 32492087.973676823\n",
      "152 32492087.973676823\n",
      "153 32492087.973676823\n",
      "154 32492087.973676823\n",
      "155 32492087.973676823\n",
      "156 32492087.973676823\n",
      "157 32492087.973676823\n",
      "158 32492087.973676823\n",
      "159 32492087.973676823\n",
      "160 32492087.973676823\n",
      "161 32492087.973676823\n",
      "162 32492087.973676823\n",
      "163 32492087.973676823\n",
      "164 32492087.973676823\n",
      "165 32492087.973676823\n",
      "166 32492087.973676823\n",
      "167 32492087.973676823\n",
      "168 32492087.973676823\n",
      "169 32492087.973676823\n",
      "170 32492087.973676823\n",
      "171 32492087.973676823\n",
      "172 32492087.973676823\n",
      "173 32492087.973676823\n",
      "174 32492087.973676823\n",
      "175 32492087.973676823\n",
      "176 32492087.973676823\n",
      "177 32492087.973676823\n",
      "178 32492087.973676823\n",
      "179 32492087.973676823\n",
      "180 32492087.973676823\n",
      "181 32492087.973676823\n",
      "182 32492087.973676823\n",
      "183 32492087.973676823\n",
      "184 32492087.973676823\n",
      "185 32492087.973676823\n",
      "186 32492087.973676823\n",
      "187 32492087.973676823\n",
      "188 32492087.973676823\n",
      "189 32492087.973676823\n",
      "190 32492087.973676823\n",
      "191 32492087.973676823\n",
      "192 32492087.973676823\n",
      "193 32492087.973676823\n",
      "194 32492087.973676823\n",
      "195 32492087.973676823\n",
      "196 32492087.973676823\n",
      "197 32492087.973676823\n",
      "198 32492087.973676823\n",
      "199 32492087.973676823\n",
      "200 32492087.973676823\n",
      "201 32492087.973676823\n",
      "202 32492087.973676823\n",
      "203 32492087.973676823\n",
      "204 32492087.973676823\n",
      "205 32492087.973676823\n",
      "206 32492087.973676823\n",
      "207 32492087.973676823\n",
      "208 32492087.973676823\n",
      "209 32492087.973676823\n",
      "210 32492087.973676823\n",
      "211 32492087.973676823\n",
      "212 32492087.973676823\n",
      "213 32492087.973676823\n",
      "214 32492087.973676823\n",
      "215 32492087.973676823\n",
      "216 32492087.973676823\n",
      "217 32492087.973676823\n",
      "218 32492087.973676823\n",
      "219 32492087.973676823\n",
      "220 32492087.973676823\n",
      "221 32492087.973676823\n",
      "222 32492087.973676823\n",
      "223 32492087.973676823\n",
      "224 32492087.973676823\n",
      "225 32492087.973676823\n",
      "226 32492087.973676823\n",
      "227 32492087.973676823\n",
      "228 32492087.973676823\n",
      "229 32492087.973676823\n",
      "230 32492087.973676823\n",
      "231 32492087.973676823\n",
      "232 32492087.973676823\n",
      "233 32492087.973676823\n",
      "234 32492087.973676823\n",
      "235 32492087.973676823\n",
      "236 32492087.973676823\n",
      "237 32492087.973676823\n",
      "238 32492087.973676823\n",
      "239 32492087.973676823\n",
      "240 32492087.973676823\n",
      "241 32492087.973676823\n",
      "242 32492087.973676823\n",
      "243 32492087.973676823\n",
      "244 32492087.973676823\n",
      "245 32492087.973676823\n",
      "246 32492087.973676823\n",
      "247 32492087.973676823\n",
      "248 32492087.973676823\n",
      "249 32492087.973676823\n",
      "250 32492087.973676823\n",
      "251 32492087.973676823\n",
      "252 32492087.973676823\n",
      "253 32492087.973676823\n",
      "254 32492087.973676823\n",
      "255 32492087.973676823\n",
      "256 32492087.973676823\n",
      "257 32492087.973676823\n",
      "258 32492087.973676823\n",
      "259 32492087.973676823\n",
      "260 32492087.973676823\n",
      "261 32492087.973676823\n",
      "262 32492087.973676823\n",
      "263 32492087.973676823\n",
      "264 32492087.973676823\n",
      "265 32492087.973676823\n",
      "266 32492087.973676823\n",
      "267 32492087.973676823\n",
      "268 32492087.973676823\n",
      "269 32492087.973676823\n",
      "270 32492087.973676823\n",
      "271 32492087.973676823\n",
      "272 32492087.973676823\n",
      "273 32492087.973676823\n",
      "274 32492087.973676823\n",
      "275 32492087.973676823\n",
      "276 32492087.973676823\n",
      "277 32492087.973676823\n",
      "278 32492087.973676823\n",
      "279 32492087.973676823\n",
      "280 32492087.973676823\n",
      "281 32492087.973676823\n",
      "282 32492087.973676823\n",
      "283 32492087.973676823\n",
      "284 32492087.973676823\n",
      "285 32492087.973676823\n",
      "286 32492087.973676823\n",
      "287 32492087.973676823\n",
      "288 32492087.973676823\n",
      "289 32492087.973676823\n",
      "290 32492087.973676823\n",
      "291 32492087.973676823\n",
      "292 32492087.973676823\n",
      "293 32492087.973676823\n",
      "294 32492087.973676823\n",
      "295 32492087.973676823\n",
      "296 32492087.973676823\n",
      "297 32492087.973676823\n",
      "298 32492087.973676823\n",
      "299 32492087.973676823\n",
      "300 32492087.973676823\n",
      "301 32492087.973676823\n",
      "302 32492087.973676823\n",
      "303 32492087.973676823\n",
      "304 32492087.973676823\n",
      "305 32492087.973676823\n",
      "306 32492087.973676823\n",
      "307 32492087.973676823\n",
      "308 32492087.973676823\n",
      "309 32492087.973676823\n",
      "310 32492087.973676823\n",
      "311 32492087.973676823\n",
      "312 32492087.973676823\n",
      "313 32492087.973676823\n",
      "314 32492087.973676823\n",
      "315 32492087.973676823\n",
      "316 32492087.973676823\n",
      "317 32492087.973676823\n",
      "318 32492087.973676823\n",
      "319 32492087.973676823\n",
      "320 32492087.973676823\n",
      "321 32492087.973676823\n",
      "322 32492087.973676823\n",
      "323 32492087.973676823\n",
      "324 32492087.973676823\n",
      "325 32492087.973676823\n",
      "326 32492087.973676823\n",
      "327 32492087.973676823\n",
      "328 32492087.973676823\n",
      "329 32492087.973676823\n",
      "330 32492087.973676823\n",
      "331 32492087.973676823\n",
      "332 32492087.973676823\n",
      "333 32492087.973676823\n",
      "334 32492087.973676823\n",
      "335 32492087.973676823\n",
      "336 32492087.973676823\n",
      "337 32492087.973676823\n",
      "338 32492087.973676823\n",
      "339 32492087.973676823\n",
      "340 32492087.973676823\n",
      "341 32492087.973676823\n",
      "342 32492087.973676823\n",
      "343 32492087.973676823\n",
      "344 32492087.973676823\n",
      "345 32492087.973676823\n",
      "346 32492087.973676823\n",
      "347 32492087.973676823\n",
      "348 32492087.973676823\n",
      "349 32492087.973676823\n",
      "350 32492087.973676823\n",
      "351 32492087.973676823\n",
      "352 32492087.973676823\n",
      "353 32492087.973676823\n",
      "354 32492087.973676823\n",
      "355 32492087.973676823\n",
      "356 32492087.973676823\n",
      "357 32492087.973676823\n",
      "358 32492087.973676823\n",
      "359 32492087.973676823\n",
      "360 32492087.973676823\n",
      "361 32492087.973676823\n",
      "362 32492087.973676823\n",
      "363 32492087.973676823\n",
      "364 32492087.973676823\n",
      "365 32492087.973676823\n",
      "366 32492087.973676823\n",
      "367 32492087.973676823\n",
      "368 32492087.973676823\n",
      "369 32492087.973676823\n",
      "370 32492087.973676823\n",
      "371 32492087.973676823\n",
      "372 32492087.973676823\n",
      "373 32492087.973676823\n",
      "374 32492087.973676823\n",
      "375 32492087.973676823\n",
      "376 32492087.973676823\n",
      "377 32492087.973676823\n",
      "378 32492087.973676823\n",
      "379 32492087.973676823\n",
      "380 32492087.973676823\n",
      "381 32492087.973676823\n",
      "382 32492087.973676823\n",
      "383 32492087.973676823\n",
      "384 32492087.973676823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385 32492087.973676823\n",
      "386 32492087.973676823\n",
      "387 32492087.973676823\n",
      "388 32492087.973676823\n",
      "389 32492087.973676823\n",
      "390 32492087.973676823\n",
      "391 32492087.973676823\n",
      "392 32492087.973676823\n",
      "393 32492087.973676823\n",
      "394 32492087.973676823\n",
      "395 32492087.973676823\n",
      "396 32492087.973676823\n",
      "397 32492087.973676823\n",
      "398 32492087.973676823\n",
      "399 32492087.973676823\n",
      "400 32492087.973676823\n",
      "401 32492087.973676823\n",
      "402 32492087.973676823\n",
      "403 32492087.973676823\n",
      "404 32492087.973676823\n",
      "405 32492087.973676823\n",
      "406 32492087.973676823\n",
      "407 32492087.973676823\n",
      "408 32492087.973676823\n",
      "409 32492087.973676823\n",
      "410 32492087.973676823\n",
      "411 32492087.973676823\n",
      "412 32492087.973676823\n",
      "413 32492087.973676823\n",
      "414 32492087.973676823\n",
      "415 32492087.973676823\n",
      "416 32492087.973676823\n",
      "417 32492087.973676823\n",
      "418 32492087.973676823\n",
      "419 32492087.973676823\n",
      "420 32492087.973676823\n",
      "421 32492087.973676823\n",
      "422 32492087.973676823\n",
      "423 32492087.973676823\n",
      "424 32492087.973676823\n",
      "425 32492087.973676823\n",
      "426 32492087.973676823\n",
      "427 32492087.973676823\n",
      "428 32492087.973676823\n",
      "429 32492087.973676823\n",
      "430 32492087.973676823\n",
      "431 32492087.973676823\n",
      "432 32492087.973676823\n",
      "433 32492087.973676823\n",
      "434 32492087.973676823\n",
      "435 32492087.973676823\n",
      "436 32492087.973676823\n",
      "437 32492087.973676823\n",
      "438 32492087.973676823\n",
      "439 32492087.973676823\n",
      "440 32492087.973676823\n",
      "441 32492087.973676823\n",
      "442 32492087.973676823\n",
      "443 32492087.973676823\n",
      "444 32492087.973676823\n",
      "445 32492087.973676823\n",
      "446 32492087.973676823\n",
      "447 32492087.973676823\n",
      "448 32492087.973676823\n",
      "449 32492087.973676823\n",
      "450 32492087.973676823\n",
      "451 32492087.973676823\n",
      "452 32492087.973676823\n",
      "453 32492087.973676823\n",
      "454 32492087.973676823\n",
      "455 32492087.973676823\n",
      "456 32492087.973676823\n",
      "457 32492087.973676823\n",
      "458 32492087.973676823\n",
      "459 32492087.973676823\n",
      "460 32492087.973676823\n",
      "461 32492087.973676823\n",
      "462 32492087.973676823\n",
      "463 32492087.973676823\n",
      "464 32492087.973676823\n",
      "465 32492087.973676823\n",
      "466 32492087.973676823\n",
      "467 32492087.973676823\n",
      "468 32492087.973676823\n",
      "469 32492087.973676823\n",
      "470 32492087.973676823\n",
      "471 32492087.973676823\n",
      "472 32492087.973676823\n",
      "473 32492087.973676823\n",
      "474 32492087.973676823\n",
      "475 32492087.973676823\n",
      "476 32492087.973676823\n",
      "477 32492087.973676823\n",
      "478 32492087.973676823\n",
      "479 32492087.973676823\n",
      "480 32492087.973676823\n",
      "481 32492087.973676823\n",
      "482 32492087.973676823\n",
      "483 32492087.973676823\n",
      "484 32492087.973676823\n",
      "485 32492087.973676823\n",
      "486 32492087.973676823\n",
      "487 32492087.973676823\n",
      "488 32492087.973676823\n",
      "489 32492087.973676823\n",
      "490 32492087.973676823\n",
      "491 32492087.973676823\n",
      "492 32492087.973676823\n",
      "493 32492087.973676823\n",
      "494 32492087.973676823\n",
      "495 32492087.973676823\n",
      "496 32492087.973676823\n",
      "497 32492087.973676823\n",
      "498 32492087.973676823\n",
      "499 32492087.973676823\n"
     ]
    }
   ],
   "source": [
    "# Numpy 확인\n",
    "import numpy as np\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 무작위의 입력과 출력 데이터를 생성합니다.\n",
    "x = np.random.randn(N, D_in) # 평균0, 표준편차1의 가우시안 표준정규분포 난수를 matrix array로 생성\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# 무작위로 가중치를 초기화합니다.\n",
    "w1 = np.random.randn(D_in, H) \n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    # 순전파 단계: 예측값 y를 계산합니다.\n",
    "    h = x.dot(w1) # x와 w1 곱셈. # x(input값)와 w1(가중치)를 곱함\n",
    "    h_relu = np.maximum(h, 0) # https://stackoverflow.com/questions/33569668/numpy-max-vs-amax-vs-maximum\n",
    "    y_pred = h_relu.dot(w2) # relu은닉층을 거쳐서 나온 값과 w2(가중치)를 곱하여 예측값 추출\n",
    "\n",
    "    # 손실(loss)을 계산하고 확인합니다.\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "    \n",
    "    # 손실에 따른 w1, w2의 변화도를 계산하고 역전파합니다.\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # 가중치를 갱신합니다.\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n",
      "tensor(34089088.)\n"
     ]
    }
   ],
   "source": [
    "# PyTorch: Tensors\n",
    "# Tensors = Numpy array (동일)\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # GPU에서 실행하려면 이 주석을 제거하세요.\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 가중치를 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 순전파 단계: 예측값 y를 계산합니다.\n",
    "    h = x.mm(w1)  # matmul하여 h layer 생성\n",
    "    h_relu = h.clamp(min=0) # ReLU : 0보다 작은 입력값은 0으로 만드는 함수. 필요없는 가중치를 지워버린다\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # 손실(loss)을 계산하고 출력합니다.\n",
    "    loss = (y_pred - y).pow(2).sum().item()   # loss변수 다음 예제에서 사용됨\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # 손실에 따른 w1, w2의 변화도를 계산하고 역전파합니다.\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # 경사하강법(gradient descent)를 사용하여 가중치를 갱신합니다.\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-8cfd50d3bfee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-6\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0msince\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m# 순전파 단계: Tensor 연산을 사용하여 예상되는 y 값을 계산합니다. 이는 Tensor를\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "#Pytorch : autograd\n",
    "\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # GPU에서 실행하려면 이 주석을 제거하세요.\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "# requires_grad=False로 설정하여 역전파 중에 이 Tensor들에 대한 변화도를 계산할\n",
    "# 필요가 없음을 나타냅니다. (requres_grad의 기본값이 False이므로 아래 코드에는\n",
    "# 이를 반영하지 않았습니다.)\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 가중치를 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "# requires_grad=True로 설정하여 역전파 중에 이 Tensor들에 대한\n",
    "# 변화도를 계산할 필요가 있음을 나타냅니다.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True) ## 파라미터 추적\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True) ## 파라미터 추적\n",
    "\n",
    "learning_rate = 1e-6\n",
    "since = time.time()\n",
    "for t in range(500):\n",
    "    # 순전파 단계: Tensor 연산을 사용하여 예상되는 y 값을 계산합니다. 이는 Tensor를\n",
    "    # 사용한 순전파 단계와 완전히 동일하지만, 역전파 단계를 별도로 구현하지 않아도\n",
    "    # 되므로 중간값들에 대한 참조(reference)를 갖고 있을 필요가 없습니다. (변수 하나로 처리)\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Tensor 연산을 사용하여 손실을 계산하고 출력합니다.\n",
    "    # loss는 (1,) 형태의 Tensor이며, loss.item()은 loss의 스칼라 값입니다.\n",
    "    loss = (y_pred - y).pow(2).sum() # item() 생략\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # autograd를 사용하여 역전파 단계를 계산합니다. 이는 requires_grad=True를\n",
    "    # 갖는 모든 Tensor에 대해 손실의 변화도를 계산합니다. 이후 w1.grad와 w2.grad는\n",
    "    # w1과 w2 각각에 대한 손실의 변화도를 갖는 Tensor가 됩니다.\n",
    "    loss.backward()  # loss들만 알면 계산식 없이 알아서 해주네...\n",
    "\n",
    "    # 경사하강법(gradient descent)을 사용하여 가중치를 수동으로 갱신합니다.\n",
    "    # torch.no_grad()로 감싸는 이유는 가중치들이 requires_grad=True이지만\n",
    "    # autograd에서는 이를 추적할 필요가 없기 때문입니다.\n",
    "    # 다른 방법은 weight.data 및 weight.grad.data를 조작하는 방법입니다.\n",
    "    # tensor.data가 tensor의 저장공간을 공유하기는 하지만, 이력을\n",
    "    # 추적하지 않는다는 것을 기억하십시오.\n",
    "    # 또한, 이를 위해 torch.optim.SGD 를 사용할 수도 있습니다.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # 가중치 갱신 후에는 수동으로 변화도를 0으로 만듭니다.\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "\n",
    "now = time.time()\n",
    "print(now - since)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## autograd 함수화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 889.4566040039062\n",
      "199 24.183610916137695\n",
      "299 0.9068278670310974\n",
      "399 0.03599667549133301\n",
      "499 0.0018360874382779002\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    pytorch에 내장된 \n",
    "    torch.autograd.Function을 상속받아 사용자 정의 autograd Function을 구현하고,\n",
    "    Tensor 연산을 하는 순전파와 역전파 단계를 구현하겠습니다.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod  # 함수 재정의\n",
    "    def forward(ctx, input):  # input : x.mm(w1)\n",
    "        \"\"\"\n",
    "        순전파 단계에서는 입력을 갖는 Tensor를 받아 출력을 갖는 Tensor를 반환합니다.\n",
    "        ctx는 컨텍스트 객체(context object)로 역전파 연산을 위한 정보 저장에\n",
    "        사용합니다. ctx.save_for_backward method를 사용하여 역전파 단계에서 사용할 어떠한\n",
    "        객체도 저장(cache)해 둘 수 있습니다.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)  # return : x.mm(w1).clamp(min=0)\n",
    "\n",
    "    @staticmethod  # 함수 재정의\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        역전파 단계에서는 출력에 대한 손실의 변화도를 갖는 Tensor를 받고, 입력에\n",
    "        대한 손실의 변화도를 계산합니다.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors  # forward에서 저장된 컨텍스트를 load\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # GPU에서 실행하려면 이 주석을 제거하세요.\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 가중치를 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 사용자 정의 Function을 적용하기 위해 Function.apply 메소드를 사용합니다.\n",
    "    # 여기에 'relu'라는 이름을 붙였습니다.\n",
    "    relu = MyReLU.apply ## 적용된다?\n",
    "\n",
    "    # 순전파 단계: Tensor 연산을 사용하여 예상되는 y 값을 계산합니다;\n",
    "    # 사용자 정의 autograd 연산을 사용하여 ReLU를 계산합니다.\n",
    "    y_pred = relu(x.mm(w1)).mm(w2) ## relu input변수로 들어간 후 계산되어 mm(w2) 수행\n",
    "\n",
    "    # 손실을 계산하고 출력합니다.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # autograde를 사용하여 역전파 단계를 계산합니다.\n",
    "    loss.backward()  # backward()\n",
    "\n",
    "    # 경사하강법(gradient descent)을 사용하여 가중치를 갱신합니다.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # 가중치 갱신 후에는 수동으로 변화도를 0으로 만듭니다.\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn모듈\n",
    "\n",
    "- PyTorch의 autograd 기능은 복잡한 연산자를 정의하고 유도해내는데 편하지만, 큰 뉴럴 네트워크에서는 구성하는데 어려움이 많다.\n",
    "\n",
    "- TensorFlow에서는 이러한 어려움을 극복하기 위해 Keras, TensorFlow-Slim, TFLearn등을 사용한다.\n",
    "\n",
    "- PyTorch에서는 동일한 기능을 위해 nn module를 구현하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nn모듈\n",
    "import torch\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# nn 패키지를 사용하여 모델을 순차적 계층(sequence of layers)으로 정의합니다.\n",
    "# nn.Sequential은 다른 Module들을 포함하는 Module로, 그 Module들을 순차적으로\n",
    "# 적용하여 출력을 생성합니다. 각각의 Linear Module은 선형 함수를 사용하여\n",
    "# 입력으로부터 출력을 계산하고, 내부 Tensor에 가중치와 편향을 저장합니다.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# 또한 nn 패키지에는 널리 사용하는 손실 함수들에 대한 정의도 포함하고 있습니다;\n",
    "# 여기에서는 평균 제곱 오차(MSE; Mean Squared Error)를 손실 함수로 사용하겠습니다.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # 순전파 단계: 모델에 x를 전달하여 예상되는 y 값을 계산합니다. Module 객체는\n",
    "    # __call__ 연산자를 덮어써(override) 함수처럼 호출할 수 있게 합니다.\n",
    "    # 이렇게 함으로써 입력 데이터의 Tensor를 Module에 전달하여 출력 데이터의\n",
    "    # Tensor를 생성합니다.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 손실을 계산하고 출력합니다. 예측한 y와 정답인 y를 갖는 Tensor들을 전달하고,\n",
    "    # 손실 함수는 손실 값을 갖는 Tensor를 반환합니다.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 역전파 단계를 실행하기 전에 변화도를 0으로 만듭니다.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # 역전파 단계: 모델의 학습 가능한 모든 매개변수에 대해 손실의 변화도를\n",
    "    # 계산합니다. 내부적으로 각 Module의 매개변수는 requires_grad=True 일 때\n",
    "    # Tensor 내에 저장되므로, 이 호출은 모든 모델의 모든 학습 가능한 매개변수의\n",
    "    # 변화도를 계산하게 됩니다.\n",
    "    loss.backward()\n",
    "\n",
    "    # 경사하강법(gradient descent)를 사용하여 가중치를 갱신합니다. 각 매개변수는\n",
    "    # Tensor이므로 이전에 했던 것과 같이 변화도에 접근할 수 있습니다.\n",
    "    with torch.no_grad(): # torch.no_grad()를 사용하여 연산의 추적을 피한다.\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad # 계산된 parameter들의 loss의 gradient에 learning_rate를 곱한 값을 현재의 가중치에서 빼줌으로 학습을 진행한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
