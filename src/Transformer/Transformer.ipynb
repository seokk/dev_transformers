{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 2.097486\n",
      "Epoch: 0002 cost = 2.939752\n",
      "Epoch: 0003 cost = 2.898980\n",
      "Epoch: 0004 cost = 5.177536\n",
      "Epoch: 0005 cost = 4.415298\n",
      "Epoch: 0006 cost = 4.616985\n",
      "Epoch: 0007 cost = 3.855356\n",
      "Epoch: 0008 cost = 4.307350\n",
      "Epoch: 0009 cost = 3.633252\n",
      "Epoch: 0010 cost = 2.569031\n",
      "Epoch: 0011 cost = 1.934179\n",
      "Epoch: 0012 cost = 1.860285\n",
      "Epoch: 0013 cost = 1.918555\n",
      "Epoch: 0014 cost = 1.999369\n",
      "Epoch: 0015 cost = 1.982306\n",
      "Epoch: 0016 cost = 1.911499\n",
      "Epoch: 0017 cost = 1.833033\n",
      "Epoch: 0018 cost = 1.741959\n",
      "Epoch: 0019 cost = 1.664004\n",
      "Epoch: 0020 cost = 1.637478\n",
      "Epoch: 0021 cost = 1.661365\n",
      "Epoch: 0022 cost = 1.701457\n",
      "Epoch: 0023 cost = 1.726670\n",
      "Epoch: 0024 cost = 1.726758\n",
      "Epoch: 0025 cost = 1.707675\n",
      "Epoch: 0026 cost = 1.680403\n",
      "Epoch: 0027 cost = 1.654493\n",
      "Epoch: 0028 cost = 1.635760\n",
      "Epoch: 0029 cost = 1.626521\n",
      "Epoch: 0030 cost = 1.625944\n",
      "Epoch: 0031 cost = 1.631008\n",
      "Epoch: 0032 cost = 1.638095\n",
      "Epoch: 0033 cost = 1.643959\n",
      "Epoch: 0034 cost = 1.645875\n",
      "Epoch: 0035 cost = 1.642315\n",
      "Epoch: 0036 cost = 1.634053\n",
      "Epoch: 0037 cost = 1.624145\n",
      "Epoch: 0038 cost = 1.616310\n",
      "Epoch: 0039 cost = 1.612892\n",
      "Epoch: 0040 cost = 1.613893\n",
      "Epoch: 0041 cost = 1.617503\n",
      "Epoch: 0042 cost = 1.621361\n",
      "Epoch: 0043 cost = 1.623644\n",
      "Epoch: 0044 cost = 1.623582\n",
      "Epoch: 0045 cost = 1.621425\n",
      "Epoch: 0046 cost = 1.618097\n",
      "Epoch: 0047 cost = 1.614727\n",
      "Epoch: 0048 cost = 1.612247\n",
      "Epoch: 0049 cost = 1.611150\n",
      "Epoch: 0050 cost = 1.611417\n",
      "Epoch: 0051 cost = 1.612591\n",
      "Epoch: 0052 cost = 1.613959\n",
      "Epoch: 0053 cost = 1.614814\n",
      "Epoch: 0054 cost = 1.614743\n",
      "Epoch: 0055 cost = 1.613781\n",
      "Epoch: 0056 cost = 1.612354\n",
      "Epoch: 0057 cost = 1.611032\n",
      "Epoch: 0058 cost = 1.610251\n",
      "Epoch: 0059 cost = 1.610154\n",
      "Epoch: 0060 cost = 1.610587\n",
      "Epoch: 0061 cost = 1.611219\n",
      "Epoch: 0062 cost = 1.611703\n",
      "Epoch: 0063 cost = 1.611811\n",
      "Epoch: 0064 cost = 1.611505\n",
      "Epoch: 0065 cost = 1.610931\n",
      "Epoch: 0066 cost = 1.610327\n",
      "Epoch: 0067 cost = 1.609918\n",
      "Epoch: 0068 cost = 1.609816\n",
      "Epoch: 0069 cost = 1.609987\n",
      "Epoch: 0070 cost = 1.610283\n",
      "Epoch: 0071 cost = 1.610521\n",
      "Epoch: 0072 cost = 1.610575\n",
      "Epoch: 0073 cost = 1.610425\n",
      "Epoch: 0074 cost = 1.610152\n",
      "Epoch: 0075 cost = 1.609887\n",
      "Epoch: 0076 cost = 1.609735\n",
      "Epoch: 0077 cost = 1.609731\n",
      "Epoch: 0078 cost = 1.609837\n",
      "Epoch: 0079 cost = 1.609968\n",
      "Epoch: 0080 cost = 1.610046\n",
      "Epoch: 0081 cost = 1.610032\n",
      "Epoch: 0082 cost = 1.609940\n",
      "Epoch: 0083 cost = 1.609816\n",
      "Epoch: 0084 cost = 1.609718\n",
      "Epoch: 0085 cost = 1.609679\n",
      "Epoch: 0086 cost = 1.609702\n",
      "Epoch: 0087 cost = 1.609757\n",
      "Epoch: 0088 cost = 1.609805\n",
      "Epoch: 0089 cost = 1.609816\n",
      "Epoch: 0090 cost = 1.609786\n",
      "Epoch: 0091 cost = 1.609732\n",
      "Epoch: 0092 cost = 1.609682\n",
      "Epoch: 0093 cost = 1.609656\n",
      "Epoch: 0094 cost = 1.609659\n",
      "Epoch: 0095 cost = 1.609680\n",
      "Epoch: 0096 cost = 1.609702\n",
      "Epoch: 0097 cost = 1.609708\n",
      "Epoch: 0098 cost = 1.609697\n",
      "Epoch: 0099 cost = 1.609674\n",
      "Epoch: 0100 cost = 1.609650\n",
      "Epoch: 0101 cost = 1.609637\n",
      "Epoch: 0102 cost = 1.609636\n",
      "Epoch: 0103 cost = 1.609643\n",
      "Epoch: 0104 cost = 1.609651\n",
      "Epoch: 0105 cost = 1.609653\n",
      "Epoch: 0106 cost = 1.609647\n",
      "Epoch: 0107 cost = 1.609636\n",
      "Epoch: 0108 cost = 1.609626\n",
      "Epoch: 0109 cost = 1.609619\n",
      "Epoch: 0110 cost = 1.609617\n",
      "Epoch: 0111 cost = 1.609619\n",
      "Epoch: 0112 cost = 1.609621\n",
      "Epoch: 0113 cost = 1.609621\n",
      "Epoch: 0114 cost = 1.609617\n",
      "Epoch: 0115 cost = 1.609612\n",
      "Epoch: 0116 cost = 1.609607\n",
      "Epoch: 0117 cost = 1.609603\n",
      "Epoch: 0118 cost = 1.609601\n",
      "Epoch: 0119 cost = 1.609601\n",
      "Epoch: 0120 cost = 1.609601\n",
      "Epoch: 0121 cost = 1.609599\n",
      "Epoch: 0122 cost = 1.609597\n",
      "Epoch: 0123 cost = 1.609594\n",
      "Epoch: 0124 cost = 1.609591\n",
      "Epoch: 0125 cost = 1.609588\n",
      "Epoch: 0126 cost = 1.609587\n",
      "Epoch: 0127 cost = 1.609586\n",
      "Epoch: 0128 cost = 1.609585\n",
      "Epoch: 0129 cost = 1.609583\n",
      "Epoch: 0130 cost = 1.609581\n",
      "Epoch: 0131 cost = 1.609579\n",
      "Epoch: 0132 cost = 1.609577\n",
      "Epoch: 0133 cost = 1.609575\n",
      "Epoch: 0134 cost = 1.609574\n",
      "Epoch: 0135 cost = 1.609573\n",
      "Epoch: 0136 cost = 1.609571\n",
      "Epoch: 0137 cost = 1.609570\n",
      "Epoch: 0138 cost = 1.609568\n",
      "Epoch: 0139 cost = 1.609566\n",
      "Epoch: 0140 cost = 1.609564\n",
      "Epoch: 0141 cost = 1.609563\n",
      "Epoch: 0142 cost = 1.609562\n",
      "Epoch: 0143 cost = 1.609561\n",
      "Epoch: 0144 cost = 1.609559\n",
      "Epoch: 0145 cost = 1.609558\n",
      "Epoch: 0146 cost = 1.609556\n",
      "Epoch: 0147 cost = 1.609555\n",
      "Epoch: 0148 cost = 1.609554\n",
      "Epoch: 0149 cost = 1.609552\n",
      "Epoch: 0150 cost = 1.609551\n",
      "Epoch: 0151 cost = 1.609550\n",
      "Epoch: 0152 cost = 1.609549\n",
      "Epoch: 0153 cost = 1.609547\n",
      "Epoch: 0154 cost = 1.609546\n",
      "Epoch: 0155 cost = 1.609545\n",
      "Epoch: 0156 cost = 1.609544\n",
      "Epoch: 0157 cost = 1.609543\n",
      "Epoch: 0158 cost = 1.609542\n",
      "Epoch: 0159 cost = 1.609541\n",
      "Epoch: 0160 cost = 1.609539\n",
      "Epoch: 0161 cost = 1.609538\n",
      "Epoch: 0162 cost = 1.609537\n",
      "Epoch: 0163 cost = 1.609536\n",
      "Epoch: 0164 cost = 1.609535\n",
      "Epoch: 0165 cost = 1.609534\n",
      "Epoch: 0166 cost = 1.609533\n",
      "Epoch: 0167 cost = 1.609532\n",
      "Epoch: 0168 cost = 1.609531\n",
      "Epoch: 0169 cost = 1.609530\n",
      "Epoch: 0170 cost = 1.609529\n",
      "Epoch: 0171 cost = 1.609528\n",
      "Epoch: 0172 cost = 1.609527\n",
      "Epoch: 0173 cost = 1.609526\n",
      "Epoch: 0174 cost = 1.609525\n",
      "Epoch: 0175 cost = 1.609524\n",
      "Epoch: 0176 cost = 1.609523\n",
      "Epoch: 0177 cost = 1.609522\n",
      "Epoch: 0178 cost = 1.609521\n",
      "Epoch: 0179 cost = 1.609521\n",
      "Epoch: 0180 cost = 1.609520\n",
      "Epoch: 0181 cost = 1.609519\n",
      "Epoch: 0182 cost = 1.609518\n",
      "Epoch: 0183 cost = 1.609517\n",
      "Epoch: 0184 cost = 1.609516\n",
      "Epoch: 0185 cost = 1.609515\n",
      "Epoch: 0186 cost = 1.609515\n",
      "Epoch: 0187 cost = 1.609514\n",
      "Epoch: 0188 cost = 1.609513\n",
      "Epoch: 0189 cost = 1.609512\n",
      "Epoch: 0190 cost = 1.609511\n",
      "Epoch: 0191 cost = 1.609510\n",
      "Epoch: 0192 cost = 1.609510\n",
      "Epoch: 0193 cost = 1.609509\n",
      "Epoch: 0194 cost = 1.609508\n",
      "Epoch: 0195 cost = 1.609507\n",
      "Epoch: 0196 cost = 1.609507\n",
      "Epoch: 0197 cost = 1.609506\n",
      "Epoch: 0198 cost = 1.609505\n",
      "Epoch: 0199 cost = 1.609504\n",
      "Epoch: 0200 cost = 1.609504\n",
      "Epoch: 0201 cost = 1.609503\n",
      "Epoch: 0202 cost = 1.609502\n",
      "Epoch: 0203 cost = 1.609502\n",
      "Epoch: 0204 cost = 1.609501\n",
      "Epoch: 0205 cost = 1.609501\n",
      "Epoch: 0206 cost = 1.609500\n",
      "Epoch: 0207 cost = 1.609499\n",
      "Epoch: 0208 cost = 1.609498\n",
      "Epoch: 0209 cost = 1.609498\n",
      "Epoch: 0210 cost = 1.609497\n",
      "Epoch: 0211 cost = 1.609496\n",
      "Epoch: 0212 cost = 1.609496\n",
      "Epoch: 0213 cost = 1.609495\n",
      "Epoch: 0214 cost = 1.609495\n",
      "Epoch: 0215 cost = 1.609494\n",
      "Epoch: 0216 cost = 1.609493\n",
      "Epoch: 0217 cost = 1.609493\n",
      "Epoch: 0218 cost = 1.609492\n",
      "Epoch: 0219 cost = 1.609492\n",
      "Epoch: 0220 cost = 1.609491\n",
      "Epoch: 0221 cost = 1.609491\n",
      "Epoch: 0222 cost = 1.609490\n",
      "Epoch: 0223 cost = 1.609489\n",
      "Epoch: 0224 cost = 1.609489\n",
      "Epoch: 0225 cost = 1.609488\n",
      "Epoch: 0226 cost = 1.609488\n",
      "Epoch: 0227 cost = 1.609487\n",
      "Epoch: 0228 cost = 1.609486\n",
      "Epoch: 0229 cost = 1.609486\n",
      "Epoch: 0230 cost = 1.609485\n",
      "Epoch: 0231 cost = 1.609485\n",
      "Epoch: 0232 cost = 1.609484\n",
      "Epoch: 0233 cost = 1.609484\n",
      "Epoch: 0234 cost = 1.609483\n",
      "Epoch: 0235 cost = 1.609483\n",
      "Epoch: 0236 cost = 1.609482\n",
      "Epoch: 0237 cost = 1.609482\n",
      "Epoch: 0238 cost = 1.609481\n",
      "Epoch: 0239 cost = 1.609481\n",
      "Epoch: 0240 cost = 1.609480\n",
      "Epoch: 0241 cost = 1.609480\n",
      "Epoch: 0242 cost = 1.609479\n",
      "Epoch: 0243 cost = 1.609479\n",
      "Epoch: 0244 cost = 1.609478\n",
      "Epoch: 0245 cost = 1.609478\n",
      "Epoch: 0246 cost = 1.609477\n",
      "Epoch: 0247 cost = 1.609477\n",
      "Epoch: 0248 cost = 1.609476\n",
      "Epoch: 0249 cost = 1.609476\n",
      "Epoch: 0250 cost = 1.609475\n",
      "Epoch: 0251 cost = 1.609475\n",
      "Epoch: 0252 cost = 1.609474\n",
      "Epoch: 0253 cost = 1.609474\n",
      "Epoch: 0254 cost = 1.609473\n",
      "Epoch: 0255 cost = 1.609473\n",
      "Epoch: 0256 cost = 1.609473\n",
      "Epoch: 0257 cost = 1.609472\n",
      "Epoch: 0258 cost = 1.609472\n",
      "Epoch: 0259 cost = 1.609471\n",
      "Epoch: 0260 cost = 1.609471\n",
      "Epoch: 0261 cost = 1.609470\n",
      "Epoch: 0262 cost = 1.609470\n",
      "Epoch: 0263 cost = 1.609469\n",
      "Epoch: 0264 cost = 1.609469\n",
      "Epoch: 0265 cost = 1.609468\n",
      "Epoch: 0266 cost = 1.609468\n",
      "Epoch: 0267 cost = 1.609468\n",
      "Epoch: 0268 cost = 1.609467\n",
      "Epoch: 0269 cost = 1.609467\n",
      "Epoch: 0270 cost = 1.609466\n",
      "Epoch: 0271 cost = 1.609466\n",
      "Epoch: 0272 cost = 1.609465\n",
      "Epoch: 0273 cost = 1.609465\n",
      "Epoch: 0274 cost = 1.609465\n",
      "Epoch: 0275 cost = 1.609464\n",
      "Epoch: 0276 cost = 1.609463\n",
      "Epoch: 0277 cost = 1.609463\n",
      "Epoch: 0278 cost = 1.609463\n",
      "Epoch: 0279 cost = 1.609462\n",
      "Epoch: 0280 cost = 1.609462\n",
      "Epoch: 0281 cost = 1.609462\n",
      "Epoch: 0282 cost = 1.609461\n",
      "Epoch: 0283 cost = 1.609461\n",
      "Epoch: 0284 cost = 1.609460\n",
      "Epoch: 0285 cost = 1.609460\n",
      "Epoch: 0286 cost = 1.609460\n",
      "Epoch: 0287 cost = 1.609459\n",
      "Epoch: 0288 cost = 1.609459\n",
      "Epoch: 0289 cost = 1.609459\n",
      "Epoch: 0290 cost = 1.609458\n",
      "Epoch: 0291 cost = 1.609458\n",
      "Epoch: 0292 cost = 1.609457\n",
      "Epoch: 0293 cost = 1.609457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0294 cost = 1.609456\n",
      "Epoch: 0295 cost = 1.609456\n",
      "Epoch: 0296 cost = 1.609456\n",
      "Epoch: 0297 cost = 1.609455\n",
      "Epoch: 0298 cost = 1.609455\n",
      "Epoch: 0299 cost = 1.609455\n",
      "Epoch: 0300 cost = 1.609454\n",
      "Epoch: 0301 cost = 1.609454\n",
      "Epoch: 0302 cost = 1.609453\n",
      "Epoch: 0303 cost = 1.609453\n",
      "Epoch: 0304 cost = 1.609452\n",
      "Epoch: 0305 cost = 1.609452\n",
      "Epoch: 0306 cost = 1.609452\n",
      "Epoch: 0307 cost = 1.609451\n",
      "Epoch: 0308 cost = 1.609451\n",
      "Epoch: 0309 cost = 1.609451\n",
      "Epoch: 0310 cost = 1.609450\n",
      "Epoch: 0311 cost = 1.609450\n",
      "Epoch: 0312 cost = 1.609450\n",
      "Epoch: 0313 cost = 1.609449\n",
      "Epoch: 0314 cost = 1.609449\n",
      "Epoch: 0315 cost = 1.609448\n",
      "Epoch: 0316 cost = 1.609448\n",
      "Epoch: 0317 cost = 1.609448\n",
      "Epoch: 0318 cost = 1.609447\n",
      "Epoch: 0319 cost = 1.609447\n",
      "Epoch: 0320 cost = 1.609447\n",
      "Epoch: 0321 cost = 1.609446\n",
      "Epoch: 0322 cost = 1.609446\n",
      "Epoch: 0323 cost = 1.609445\n",
      "Epoch: 0324 cost = 1.609445\n",
      "Epoch: 0325 cost = 1.609445\n",
      "Epoch: 0326 cost = 1.609444\n",
      "Epoch: 0327 cost = 1.609444\n",
      "Epoch: 0328 cost = 1.609443\n",
      "Epoch: 0329 cost = 1.609443\n",
      "Epoch: 0330 cost = 1.609443\n",
      "Epoch: 0331 cost = 1.609442\n",
      "Epoch: 0332 cost = 1.609442\n",
      "Epoch: 0333 cost = 1.609442\n",
      "Epoch: 0334 cost = 1.609441\n",
      "Epoch: 0335 cost = 1.609441\n",
      "Epoch: 0336 cost = 1.609440\n",
      "Epoch: 0337 cost = 1.609440\n",
      "Epoch: 0338 cost = 1.609440\n",
      "Epoch: 0339 cost = 1.609439\n",
      "Epoch: 0340 cost = 1.609439\n",
      "Epoch: 0341 cost = 1.609439\n",
      "Epoch: 0342 cost = 1.609438\n",
      "Epoch: 0343 cost = 1.609438\n",
      "Epoch: 0344 cost = 1.609437\n",
      "Epoch: 0345 cost = 1.609437\n",
      "Epoch: 0346 cost = 1.609436\n",
      "Epoch: 0347 cost = 1.609436\n",
      "Epoch: 0348 cost = 1.609436\n",
      "Epoch: 0349 cost = 1.609435\n",
      "Epoch: 0350 cost = 1.609435\n",
      "Epoch: 0351 cost = 1.609435\n",
      "Epoch: 0352 cost = 1.609434\n",
      "Epoch: 0353 cost = 1.609434\n",
      "Epoch: 0354 cost = 1.609433\n",
      "Epoch: 0355 cost = 1.609433\n",
      "Epoch: 0356 cost = 1.609433\n",
      "Epoch: 0357 cost = 1.609432\n",
      "Epoch: 0358 cost = 1.609432\n",
      "Epoch: 0359 cost = 1.609432\n",
      "Epoch: 0360 cost = 1.609432\n",
      "Epoch: 0361 cost = 1.609431\n",
      "Epoch: 0362 cost = 1.609430\n",
      "Epoch: 0363 cost = 1.609430\n",
      "Epoch: 0364 cost = 1.609430\n",
      "Epoch: 0365 cost = 1.609429\n",
      "Epoch: 0366 cost = 1.609429\n",
      "Epoch: 0367 cost = 1.609428\n",
      "Epoch: 0368 cost = 1.609428\n",
      "Epoch: 0369 cost = 1.609428\n",
      "Epoch: 0370 cost = 1.609427\n",
      "Epoch: 0371 cost = 1.609427\n",
      "Epoch: 0372 cost = 1.609426\n",
      "Epoch: 0373 cost = 1.609426\n",
      "Epoch: 0374 cost = 1.609426\n",
      "Epoch: 0375 cost = 1.609425\n",
      "Epoch: 0376 cost = 1.609425\n",
      "Epoch: 0377 cost = 1.609424\n",
      "Epoch: 0378 cost = 1.609424\n",
      "Epoch: 0379 cost = 1.609423\n",
      "Epoch: 0380 cost = 1.609423\n",
      "Epoch: 0381 cost = 1.609422\n",
      "Epoch: 0382 cost = 1.609422\n",
      "Epoch: 0383 cost = 1.609421\n",
      "Epoch: 0384 cost = 1.609421\n",
      "Epoch: 0385 cost = 1.609421\n",
      "Epoch: 0386 cost = 1.609420\n",
      "Epoch: 0387 cost = 1.609420\n",
      "Epoch: 0388 cost = 1.609419\n",
      "Epoch: 0389 cost = 1.609419\n",
      "Epoch: 0390 cost = 1.609418\n",
      "Epoch: 0391 cost = 1.609418\n",
      "Epoch: 0392 cost = 1.609417\n",
      "Epoch: 0393 cost = 1.609417\n",
      "Epoch: 0394 cost = 1.609416\n",
      "Epoch: 0395 cost = 1.609416\n",
      "Epoch: 0396 cost = 1.609415\n",
      "Epoch: 0397 cost = 1.609415\n",
      "Epoch: 0398 cost = 1.609414\n",
      "Epoch: 0399 cost = 1.609414\n",
      "Epoch: 0400 cost = 1.609414\n",
      "Epoch: 0401 cost = 1.609413\n",
      "Epoch: 0402 cost = 1.609412\n",
      "Epoch: 0403 cost = 1.609412\n",
      "Epoch: 0404 cost = 1.609411\n",
      "Epoch: 0405 cost = 1.609411\n",
      "Epoch: 0406 cost = 1.609410\n",
      "Epoch: 0407 cost = 1.609410\n",
      "Epoch: 0408 cost = 1.609409\n",
      "Epoch: 0409 cost = 1.609409\n",
      "Epoch: 0410 cost = 1.609408\n",
      "Epoch: 0411 cost = 1.609408\n",
      "Epoch: 0412 cost = 1.609407\n",
      "Epoch: 0413 cost = 1.609406\n",
      "Epoch: 0414 cost = 1.609406\n",
      "Epoch: 0415 cost = 1.609405\n",
      "Epoch: 0416 cost = 1.609405\n",
      "Epoch: 0417 cost = 1.609404\n",
      "Epoch: 0418 cost = 1.609404\n",
      "Epoch: 0419 cost = 1.609403\n",
      "Epoch: 0420 cost = 1.609402\n",
      "Epoch: 0421 cost = 1.609402\n",
      "Epoch: 0422 cost = 1.609401\n",
      "Epoch: 0423 cost = 1.609400\n",
      "Epoch: 0424 cost = 1.609400\n",
      "Epoch: 0425 cost = 1.609399\n",
      "Epoch: 0426 cost = 1.609398\n",
      "Epoch: 0427 cost = 1.609398\n",
      "Epoch: 0428 cost = 1.609397\n",
      "Epoch: 0429 cost = 1.609397\n",
      "Epoch: 0430 cost = 1.609396\n",
      "Epoch: 0431 cost = 1.609395\n",
      "Epoch: 0432 cost = 1.609394\n",
      "Epoch: 0433 cost = 1.609394\n",
      "Epoch: 0434 cost = 1.609393\n",
      "Epoch: 0435 cost = 1.609392\n",
      "Epoch: 0436 cost = 1.609392\n",
      "Epoch: 0437 cost = 1.609391\n",
      "Epoch: 0438 cost = 1.609390\n",
      "Epoch: 0439 cost = 1.609389\n",
      "Epoch: 0440 cost = 1.609388\n",
      "Epoch: 0441 cost = 1.609388\n",
      "Epoch: 0442 cost = 1.609387\n",
      "Epoch: 0443 cost = 1.609386\n",
      "Epoch: 0444 cost = 1.609385\n",
      "Epoch: 0445 cost = 1.609385\n",
      "Epoch: 0446 cost = 1.609383\n",
      "Epoch: 0447 cost = 1.609383\n",
      "Epoch: 0448 cost = 1.609382\n",
      "Epoch: 0449 cost = 1.609381\n",
      "Epoch: 0450 cost = 1.609380\n",
      "Epoch: 0451 cost = 1.609379\n",
      "Epoch: 0452 cost = 1.609378\n",
      "Epoch: 0453 cost = 1.609377\n",
      "Epoch: 0454 cost = 1.609376\n",
      "Epoch: 0455 cost = 1.609375\n",
      "Epoch: 0456 cost = 1.609374\n",
      "Epoch: 0457 cost = 1.609373\n",
      "Epoch: 0458 cost = 1.609372\n",
      "Epoch: 0459 cost = 1.609371\n",
      "Epoch: 0460 cost = 1.609370\n",
      "Epoch: 0461 cost = 1.609369\n",
      "Epoch: 0462 cost = 1.609368\n",
      "Epoch: 0463 cost = 1.609367\n",
      "Epoch: 0464 cost = 1.609366\n",
      "Epoch: 0465 cost = 1.609365\n",
      "Epoch: 0466 cost = 1.609363\n",
      "Epoch: 0467 cost = 1.609362\n",
      "Epoch: 0468 cost = 1.609361\n",
      "Epoch: 0469 cost = 1.609360\n",
      "Epoch: 0470 cost = 1.609359\n",
      "Epoch: 0471 cost = 1.609357\n",
      "Epoch: 0472 cost = 1.609356\n",
      "Epoch: 0473 cost = 1.609355\n",
      "Epoch: 0474 cost = 1.609353\n",
      "Epoch: 0475 cost = 1.609352\n",
      "Epoch: 0476 cost = 1.609350\n",
      "Epoch: 0477 cost = 1.609349\n",
      "Epoch: 0478 cost = 1.609348\n",
      "Epoch: 0479 cost = 1.609346\n",
      "Epoch: 0480 cost = 1.609344\n",
      "Epoch: 0481 cost = 1.609343\n",
      "Epoch: 0482 cost = 1.609341\n",
      "Epoch: 0483 cost = 1.609339\n",
      "Epoch: 0484 cost = 1.609337\n",
      "Epoch: 0485 cost = 1.609336\n",
      "Epoch: 0486 cost = 1.609334\n",
      "Epoch: 0487 cost = 1.609332\n",
      "Epoch: 0488 cost = 1.609330\n",
      "Epoch: 0489 cost = 1.609328\n",
      "Epoch: 0490 cost = 1.609325\n",
      "Epoch: 0491 cost = 1.609323\n",
      "Epoch: 0492 cost = 1.609320\n",
      "Epoch: 0493 cost = 1.609316\n",
      "Epoch: 0494 cost = 1.609312\n",
      "Epoch: 0495 cost = 1.609307\n",
      "Epoch: 0496 cost = 1.609301\n",
      "Epoch: 0497 cost = 1.609294\n",
      "Epoch: 0498 cost = 1.609285\n",
      "Epoch: 0499 cost = 1.609275\n",
      "Epoch: 0500 cost = 1.609264\n",
      "ich mochte ein bier P -> ['E', 'a', 'a', 'a', 'E']\n",
      "first head of last state enc_self_attns\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe4AAAIACAYAAABEsY85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAFy1JREFUeJzt3Xu0rHdd3/HPN+cEQgR0mYSGhEAk\nggXKpXgAEbmVLAO4lotFEatAixQCiJWLFrUKopbF4iaXBsRTgdAaVIq25WJFaJKKCIQAS0pTBbmG\nAObkUiAEQxJ+/WPmkO24k3POPpl55jt5vdaadfZ+ZvbJdz9r57z375lnnqkxRgCAHo6YegAA4OAJ\nNwA0ItwA0IhwA0Ajwg0AjQg3ADQi3ADQiHADQCPCDQCNCDcANCLctFZVP11V/6eqrqyqO863/WJV\nPXbq2QCWQbhpq6qeleRXkuxNUlvuuijJz0wyFMCSCTedPS3JU8YYr0pyzZbtH0lyt2lGAlgu4aaz\nOyT5+Dbbr05yixXPArASwk1nn05y7222PzLJBSueBWAldk89AByGlyU5o6qOzuw57vtX1ROSPDfJ\nkyadDGBJaowx9QywY1X1lMxOUDtpvumiJC8YY7x+uqkAlke42QhVdWySI8YYF089C8AyeY6btqrq\n7Kr6riQZY1yyP9pVdeuqOnva6QCWw4qbtqrqW0mOX1xlV9Vtklw0xjhymskAlsfJabRTVVvPJL9H\nVV225fNdSU7L7LlugI1jxU0785X2/h/c2uYh30jyb8YYb1jdVACrYcVNR9+TWbA/neS+SfZtue+b\nSS4eY1w7xWAAy2bFDQCNWHHTWlWdlOSBSW6ThVdJjDF+c5KhAJbIipu2qupxSd6Q2RuM7Mt1z3sn\nyRhj3HGSwQCWSLhpq6o+leQPkjzPc9rATYVw01ZVXZHkHmOMT089C8CquHIanf1xkvtNPQTAKjk5\njVaq6tFbPn13khdX1d2S/O/M3of728YYf7TK2QBWwaFyWplffOVgjDHGrqUOAzAB4QaARjzHDQCN\nCDdtVdUbqurnttn+nKr6nSlmAlg24aazRybZ7n23z57fB2ulqnZX1SOr6pipZ6Ev4aaz70pyxTbb\nv57ku1c8CxzQGOOaJH+U5FZTz0Jfwk1nn8j2K+sfSfI3K54FDtZfJvneqYegL6/jprOXJ3ldVd0m\n1x0yf1iSZyV5xmRTwQ17QZKXV9WvJvlwZkeIvm2McdkUQ9GHl4PRWlU9NcmvJDlxvumiJC8cY7xu\nuqng+i1ci2DrP8AV1x/gIAg3G6Gqjsvs5/niqWeBG1JVD76h+8cY/2tVs9CTcNNeVd0xyV0zW71c\nMMb4zMQjbYSqOjrJvbL9e527nCxMxHPctFVVt07y+iT/PMm3rttcf5jkX48xvjbZcM1V1alJfi/J\ndi9bGkkczj0MVXX3JE9NckqSJ40xvlRVj0ryuTHGR6edjnXnrPIVqaqjq+oHq+pRVfXorbepZ2vs\nVUnukeShSW4xvz1svu2VE861CV6V5J1JbjfGOGLhJtqHoap+OMmHMjsv459l9nObzCL+q1PNRR8O\nla/AgVYv/iHcmaq6NMmjxhjvXdj+oCT/dYzhIhc7VFVfz+y9zj819Sybpqo+mORNY4zXVtXXktxz\njPHpqvr+JG8fY5ww8YisOSvu1bB6WY5bJLl0m+2XJTlqxbNsmvcl+b6ph9hQd8vsveQXXRYXDuIg\neI57NU5O8qNjjC9OPciGeV+S36iqJ4wxrkySqvqOJL+W5C8mnay/1yV5WVWdkO3f6/wjk0y1GS7P\n7DD5Zxe23zvJF1Y+De0I92rsX7047HjjenaSP0lyUVV9LLOTpu6Z5MokPzzlYBvgrfM/925zn5PT\nDs+bk7y0qh6b2b7cPX+J2MuSvHHSyWjBc9xLUlX33vLpyUn+fZLfjNXLjaqqbpHkcUnuktkFLC5I\nctYY4xuTDtZcVd3hhu4fY3xuVbNsmqo6MsmZSf5FZj+z35r/+eYkTxxjXDvddHQg3EsyvzrSyOx/\nyBvi5LTDUFXHJ/nBbP9a49dOMhQchKo6Jck/zezn9qNjjE9OPBJNCPeSHGjFspXVy85U1eOT/E5m\nvxxdnr9/+cjh7NxDM39p4tvHGFcf6GWKLsAC0xFu2qqqzyV5U5Jfn79dIodhfpTo+DHGxQvX017k\nKNEhqqpXJ/mlMcbX5x9frzHGz65oLJpyctoKVNULk1y4+MYXVfW0JCeOMZ43zWTt3TrJmaJ94xhj\nHLHdx9wo7p7kyC0fXx8rKQ7IinsFqurzSX5sjPHBhe33SfLWMcZBH1bnOlV1RpK/HmP8h6ln2URV\n9YjM3h71jklOG2NcWFVPTvKZMcb/nHa6zVBVt0ySMcYVU89CH36rXo3bJNm3zfZLk/yjFc+ySZ6T\n5BFV9d+q6jeq6vlbb1MP11lVPS7JW5J8Msn35LrV4q4kz51qrk1RVc+a/0L/lSRfqaoLq+rZVXWg\nk1nZxvyS0q+pqouq6uKqenNVHTv1XMviUPlqfD7JA5N8emH7g+KCC4fjqUkenuSSJN+bhZPTkvz6\nFENtiOcmecoY4/fnq+z9PhD79bBU1UuSnJ7kpUneP998/yTPT3Lb+MVoJ34tyROTnJXkG0l+Mslv\nJfmxCWdaGuFejd9O8oqqulmSs+fbHpbkRUlePNlU/T0vyc+NMV4x9SAb6E65LipbXZHZuQXs3JOT\nPHmM8dYt286uqr/O7N8K4T50j87sHQF/P0mq6qwk76uqXZv4unjhXoExxsvnh21eneRmmb186arM\nrmH+0ilna25XkrdNPcSG+mKSOydZfKnig+IKgDeGj13PNk9f7sxJSb79ZkNjjPOq6pokJyS5cLKp\nlsQPyYqMMX4pybFJfmB+O26M8YvD2YGH442ZXTWNG9/eJK+uqgfMPz+pqv5VkpdkdgiSnftPmZ30\nt+jpSf7zimfZFLuSfHNh2zXZ0MXpRn5T66Cq3pbk8WOMr84/3u4xSZIxxo+ucrYNcnSSJ1fVaZmt\nVhYvJev1sDs0xnhJVX1nkndn9k5r52R2lOhlY4zXTDpcQwuv3d6d5PHzn9sPzLfdL7PV4Vmrnm1D\nVJLfraqrtmw7Ksl/rKor92/YlH9rhXt5Ls11J0tt99aTHL67JPno/ON/vHCfIxmHaYzxy/NrENw1\ns6NzF3jZ0o4tvnb7w/M/978U9Mvz2+LPMQfnTdts+92VT7EiXscNAI14jhsAGhHuFauq06eeYVPZ\nt8tj3y6Pfbs8m7pvhXv1NvIHaU3Yt8tj3y6Pfbs8G7lvhRsAGtmIk9OO/e5d4+STjjzwA9fAvkuv\nzXHHeEfEZbBvl8e+XZ5u+/YTHzt66hEO2tW5Kkfm5lOPcdC+lssvGWMcd6DHbcTLwU4+6cic966T\nph4DYOOddsK9ph5hY71nvHXxSoXbcqgcABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaE\nGwAaEW4AaES4AaAR4QaARoQbABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4A\naES4AaAR4QaARoQbABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR\n4QaARoQbABpZ63BX1ZlV9Y6p5wCAdbF76gEO4JlJauohAGBdrHW4xxhfmXoGAFgnDpUDQCNrHW4A\n4O9rG+6qOr2qzq+q8/ddeu3U4wDASrQN9xhj7xhjzxhjz3HH7Jp6HABYibbhBoCbIuEGgEaEGwAa\nEW4AaGTdL8DyxKlnAIB1YsUNAI0INwA0ItwA0IhwA0Ajwg0AjQg3ADQi3ADQiHADQCPCDQCNCDcA\nNCLcANCIcANAI8INAI0INwA0ItwA0IhwA0Ajwg0AjQg3ADQi3ADQiHADQCPCDQCNCDcANCLcANCI\ncANAI8INAI0INwA0ItwA0IhwA0Ajwg0AjQg3ADQi3ADQiHADQCPCDQCNCDcANCLcANCIcANAI8IN\nAI0INwA0ItwA0IhwA0Ajwg0AjQg3ADQi3ADQiHADQCPCDQCNCDcANCLcANCIcANAI8INAI0INwA0\nItwA0IhwA0Ajwg0AjQg3ADQi3ADQiHADQCPCDQCNCDcANCLcANCIcANAI8INAI0INwA0ItwA0Ihw\nA0Ajwg0AjQg3ADQi3ADQiHADQCPCDQCNCDcANCLcANCIcANAI8INAI0INwA0ItwA0IhwA0Ajaxnu\nqjq3qs6Yeg4AWDdrGW4AYHsHDHdVPaKqvlZVu+ef36mqRlX91pbHvLCq3l1Vu6rq9VX1mar6RlV9\nsqqeW1VHbHnsmVX1jqp6ZlVdVFWXV9Ubq+ro/fcneXCSZ8z/O6OqTr6Rv28AaGn3QTzmvUmOSrIn\nyQeSPCTJJUkeuuUxD0nyx5n9InBRkscm2Zfkvkn2Jrk0yeu3PP6BSb6U5NQkJyV5S5JPJHlRkmcm\nuXOSv0ry7+aP33eI3xcAbKQDrrjHGFck+UiuC/VDkpyR5A5Vddv5Svk+Sc4dY1w9xnj+GONDY4zP\njjHekuR1SX5i4a/9apKnjzH+7xjjT5P8lyQPm//3vpLkm0muHGN8eX67dnGuqjq9qs6vqvP3XfoP\n7gaAjXSwz3Gfm1mwk9lh7P+R5Lz5tgckuXr+earqafOg7quqK5I8O8ntF/6+C8YY12z5/ItJbnMo\ng48x9o4x9owx9hx3zK5D+VIAaOtQwv2Aqrprklsl+fB820Mzi/dfjDGurqofT/LKJGcmOS3JvZK8\nNsnNFv6+qxc+H4cwCwDcZB3Mc9zJ7Hnumyd5bpI/H2NcW1XnZvb89cWZPb+dJD+U5INjjG+/lKuq\nTtnBXN9MYhkNAAsOapW75Xnuxyc5Z775/ZmdWHa/zFbfyewEs3vPz0S/U1U9L7ND64fqs0nuW1Un\nV9WxW89KB4CbskMJ4jmZrYLPTZIxxt9ldpb5VZk/v53ktzM7Q/zNST6U5OQkL9/BXC/LbNV9QWZn\nlC8+Rw4AN0k1xph6hsO2555HjfPeddLUYwBsvNNOuNfUI2ys94y3fniMsedAj3MIGgAaEW4AaES4\nAaAR4QaARoQbABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaA\nRoQbABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoR\nbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBoRLgB\noBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBoRLgBoBHhBoBG\nhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBoRLgBoBHhBoBGhBsAGhFu\nAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBoZK3CXVUPr6r3VtXlVXVZVb2rqu4y\n9VwAsC7WKtxJviPJK5PcN8lDknwlydur6mZTDgUA62L31ANsNcb4w62fV9VPJflqZiH/84X7Tk9y\nepLc/sS1+jYAYGnWasVdVadU1Zur6lNV9dUkf5vZjLdffOwYY+8YY88YY89xx+xa+awAMIV1W6q+\nPclFSZ46//OaJBckcagcALJG4a6qY5LcJckzxhjnzLfdO2s0IwBMbZ2ieHmSS5I8paouTHJikpdm\ntuoGALJGz3GPMb6V5MeT3CPJx5O8Jsnzklw15VwAsE7WacWdMcbZSf7JwuZbTjELAKyjtVlxAwAH\nJtwA0IhwA0Ajwg0AjQg3ADQi3ADQiHADQCPCDQCNCDcANCLcANCIcANAI8INAI0INwA0ItwA0Ihw\nA0Ajwg0AjQg3ADQi3ADQiHADQCPCDQCNCDcANCLcANCIcANAI8INAI0INwA0ItwA0IhwA0Ajwg0A\njQg3ADQi3ADQiHADQCPCDQCNCDcANCLcANCIcANAI8INAI0INwA0ItwA0IhwA0Ajwg0AjQg3ADQi\n3ADQiHADQCPCDQCNCDcANCLcANCIcANAI8INAI0INwA0ItwA0IhwA0Ajwg0AjQg3ADQi3ADQiHAD\nQCPCDQCNCDcANCLcANCIcANAI8INAI0INwA0ItwA0IhwA0Ajwg0AjQg3ADQi3ADQiHADQCPCDQCN\nCDcANCLcANCIcANAI8INAI0INwA0ItwA0IhwA0AjhxTuqjq3qs5Y1jAAwA2z4gaARtY+3FV1s6ln\nAIB1sZNw766qV1XV5fPbS6vqiGQW2ap6cVV9oaq+XlUfqqrTtn5xVd21qt5ZVV+rqour6veq6vgt\n959ZVe+oql+oqi8k+cLhfYsAsDl2Eu7Hzb/u/kmemuT0JM+a3/fGJA9O8pNJ7p7kTUneXlX3TJKq\num2SP0vy8ST3TXJqklsmedv++M89OMk9kjw8ycN2MCMAbKTdO/iaLyX52THGSPJXVXXnJM+pqv+e\n5CeSnDzG+Pz8sWdU1amZBf6nkzw9yV+OMX5h/19WVf8yyWVJ9iQ5b77575I8aYxx1fUNUVWnZ/ZL\nQ25/4k6+DQDoZycr7g/Mo73f+5OcmOSHklSSC6rqiv23JD+S5JT5Y78/yYMW7r9wft8pW/7Oj99Q\ntJNkjLF3jLFnjLHnuGN27eDbAIB+buyl6khynyRXL2z/xvzPI5K8M8nPb/O1f7vl46/fyHMBwEbY\nSbjvV1W1ZdX9A0m+mNnKu5IcP8Y453q+9iNJHpvkc2OMxbgDAAewk0PlJyR5ZVV9X1U9Jsm/TfKK\nMcYnkpyV5MyqekxV3bGq9lTVz1fVo+df+5ok35nkD6rqfvPHnFpVe6vqVjfKdwQAG2wnK+6zkuxK\n8sHMDo2/Pskr5vf9VJJfTvKSJLfL7KSz85KckyRjjC9W1QOSvCjJnyQ5Ksnnk/xpkht8ThsAOMRw\njzEesuXTn9nm/quTvGB+u76/45NJHnMD9z/xUGYCgJuStb9yGgBwHeEGgEaEGwAaEW4AaES4AaAR\n4QaARoQbABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQb\nABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBo\nRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBoRLgBoBHh\nBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBoRLgBoBHhBoBGhBsA\nGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhE\nuAGgEeEGgEaEGwAaEW4AaGRtwl1VZ1bV2Ob2galnA4B1sXvqARa8J8kTFrZ9c4pBAGAdrVu4rxpj\nfHnqIQBgXa3NoXIA4MDWLdwPr6orFm4v3u6BVXV6VZ1fVefvu/TaVc8JAJNYt0Plf5bk9IVt/2+7\nB44x9ibZmyR77nnUWPJcALAW1i3cV44x/mbqIQBgXa3boXIA4Aas24r75lV1/MK2a8cY+yaZBgDW\nzLqF+9QkX1rYdlGS200wCwCsnbU5VD7GeOIYo7a5iTYAzK1NuAGAAxNuAGhEuAGgEeEGgEaEGwAa\nEW4AaES4AaAR4QaARoQbABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4\nAaAR4QaARoQbABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaA\nRoQbABoRbgBoRLgBoBHhBoBGaowx9QyHrar2Jfnc1HMcpGOTXDL1EBvKvl0e+3Z57Nvl6bZv7zDG\nOO5AD9qIcHdSVeePMfZMPccmsm+Xx75dHvt2eTZ13zpUDgCNCDcANCLcq7d36gE2mH27PPbt8ti3\ny7OR+9Zz3ADQiBU3ADQi3ADQiHADQCPCDQCNCDcANPL/AYflpOuMtLnuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27fc21ddd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first head of last state dec_self_attns\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe4AAAIACAYAAABEsY85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAF7JJREFUeJzt3Xu0pXdd3/HPNzMJJASkktAQCEQC\nWKBcCkMQkZtkGcC1XCyKWAVapDCAWLlZ1CqIWhaL+6UBcSoQWoNCo7VcrAomqYiEEGCJNCrINQQw\nk0sDuRgm4dc/9g45nnUylzNz9rO/O6/XWnvNOc/eZ/I9zzqZ9/k9+9nPrjFGAIAeDpt6AABg/wk3\nADQi3ADQiHADQCPCDQCNCDcANCLcANCIcANAI8INAI0INwA0Ity0VlU/U1X/t6qurqq7zrf9YlU9\naerZALaCcNNWVT0/ya8k2ZWk1tx1UZKfnWQogC0m3HT27CTPHGO8Mcl1a7Z/Msm9pxkJYGsJN53d\nJclnNti+J8mRC54FYCGEm86+kOQBG2x/XJILFjwLwEJsn3oAOAivSXJaVR2V2XPcD6mqpyZ5cZKn\nTzoZwBapMcbUM8CmVdUzMztB7YT5pouSvGyM8bbppgLYOsLNSqiqY5IcNsa4eOpZALaS57hpq6rO\nqqrbJskY45Ibol1Vt6mqs6adDmBrWHHTVlV9J8lx61fZVXX7JBeNMQ6fZjKArePkNNqpqrVnkt+3\nqi5b8/m2JKdm9lw3wMqx4qad+Ur7hh/c2uAh1yT5D2OMty9uKoDFsOKmo+/LLNhfSHJykt1r7vt2\nkovHGNdPMRjAVrPiBoBGrLhprapOSPKwJLfPuldJjDFeN8lQAFvIipu2qurJSd6e2RuM7M6Nz3sn\nyRhj3HWSwQC2kHDTVlV9Psm7k7zEc9rAzYVw01ZVXZnkvmOML0w9C8CiuHIanf1RkgdPPQTAIjk5\njVaq6glrPv1gkldW1b2T/HVm78P9XWOMP1jkbACL4FA5rcwvvrI/xhhj25YOAzAB4QaARjzHDQCN\nCDdtVdXbq+pFG2x/YVX99hQzAWw14aazxyXZ6H23z5rfB0ulqrZX1eOq6nZTz0Jfwk1nt01y5Qbb\nr0ryvQueBfZpjHFdkj9IcuupZ6Ev4aazz2bjlfWPJvn7Bc8C++uvktxt6iHoy+u46ey1Sd5aVbfP\njYfMH53k+UmeO9lUsHcvS/LaqvrVJJ/I7AjRd40xLptiKPrwcjBaq6pnJfmVJHecb7ooycvHGG+d\nbiq4aeuuRbD2H+CK6w+wH4SblVBVx2b283zx1LPA3lTVI/Z2/xjj/yxqFnoSbtqrqrsmuVdmq5cL\nxhhfnHiklVBVRyW5fzZ+r3OXk4WJeI6btqrqNkneluRfJ/nOjZvr95P8+zHGtyYbrrmqOiXJ7ybZ\n6GVLI4nDuQehqu6T5FlJTkry9DHG16vq8Um+PMb41LTTseycVb4gVXVUVf1gVT2+qp6w9jb1bI29\nMcl9kzwqyZHz26Pn294w4Vyr4I1JPpDkTmOMw9bdRPsgVNWPJPl4Zudl/HBmP7fJLOK/OtVc9OFQ\n+QLsa/XiH8LNqapLkzx+jPHhddsfnuR/jjFc5GKTquqqzN7r/PNTz7JqqupjSd45xnhLVX0ryf3G\nGF+oqgcmed8Y4/iJR2TJWXEvhtXL1jgyyaUbbL8syS0XPMuq+UiS7596iBV178zeS369y+LCQewH\nz3EvxolJfmyM8bWpB1kxH0nyG1X11DHG1UlSVbdK8mtJ/nLSyfp7a5LXVNXx2fi9zj85yVSr4fLM\nDpN/ad32ByT56sKnoR3hXowbVi8OOx5aL0jyx0kuqqpPZ3bS1P2SXJ3kR6YcbAWcOf9z1wb3OTnt\n4Lwryaur6kmZ7cvt85eIvSbJOyadjBY8x71FquoBaz49Mcl/TvK6WL0cUlV1ZJInJ7lnZhewuCDJ\nGWOMayYdrLmqusve7h9jfHlRs6yaqjo8yelJ/k1mP7Pfmf/5riRPG2NcP910dCDcW2R+daSR2f+Q\ne+PktINQVccl+cFs/Frjt0wyFOyHqjopyb/K7Of2U2OMz008Ek0I9xbZ14plLauXzamqpyT57cx+\nObo8//TykcPZuQdm/tLE940x9uzrZYouwALTEW7aqqovJ3lnkl+fv10iB2F+lOi4McbF666nvZ6j\nRAeoqt6U5JfGGFfNP75JY4yfW9BYNOXktAWoqpcnuXD9G19U1bOT3HGM8ZJpJmvvNklOF+1DY4xx\n2EYfc0jcJ8nhaz6+KVZS7JMV9wJU1VeS/PgY42Prtj8oyZljjP0+rM6Nquq0JH83xvgvU8+yiqrq\nsZm9Pepdk5w6xriwqp6R5ItjjD+bdrrVUFVHJ8kY48qpZ6EPv1Uvxu2T7N5g+6VJ/vmCZ1klL0zy\n2Kr6w6r6jap66drb1MN1VlVPTvKeJJ9L8n25cbW4LcmLp5prVVTV8+e/0F+R5IqqurCqXlBV+zqZ\nlQ3MLyn95qq6qKourqp3VdUxU8+1VRwqX4yvJHlYki+s2/7wuODCwXhWksckuSTJ3bLu5LQkvz7F\nUCvixUmeOcb4vfkq+wbnxn49KFX1qiQ7k7w6yUfnmx+S5KVJ7hC/GG3GryV5WpIzklyT5KeS/GaS\nH59wpi0j3IvxW0leX1VHJDlrvu3RSV6R5JWTTdXfS5K8aIzx+qkHWUF3z41RWevKzM4tYPOekeQZ\nY4wz12w7q6r+LrN/K4T7wD0hs3cE/L0kqaozknykqrat4uvihXsBxhivnR+2eVOSIzJ7+dK1mV3D\n/NVTztbctiTvnXqIFfW1JPdIsv6lig+PKwAeCp++iW2evtycE5J8982GxhjnVdV1SY5PcuFkU20R\nPyQLMsb4pSTHJPmB+e3YMcYvDmcHHox3ZHbVNA69XUneVFUPnX9+QlX9uySvyuwQJJv33zI76W+9\n5yT57wueZVVsS/Ltdduuy4ouTlfym1oGVfXeJE8ZY3xz/vFGj0mSjDF+bJGzrZCjkjyjqk7NbLWy\n/lKyXg+7SWOMV1XV9yT5YGbvtHZ2ZkeJXjPGePOkwzW07rXb25M8Zf5ze+5824MzWx2esejZVkQl\n+Z2qunbNtlsm+a9VdfUNG1bl31rh3jqX5saTpTZ660kO3j2TfGr+8b9Yd58jGQdpjPHL82sQ3Cuz\no3MXeNnSpq1/7fYn5n/e8FLQb8xv63+O2T/v3GDb7yx8igXxOm4AaMRz3ADQiHAvWFXtnHqGVWXf\nbh37duvYt1tnVfetcC/eSv4gLQn7duvYt1vHvt06K7lvhRsAGlmJk9OO+d5t48QTDt/3A5fA7kuv\nz7G36/OOiJ/99FFTj7Df9uTaHJ5bTD3GSrJvt459u3W67dtv5fJLxhjH7utxK/FysBNPODzn/ckJ\nU4+xkk49/v5TjwBws/Chceb6KxVuyKFyAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoR\nbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBoRLgB\noBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBoRLgBoBHhBoBG\nhBsAGhFuAGhkqcNdVadX1funngMAlsX2qQfYh+clqamHAIBlsdThHmNcMfUMALBMHCoHgEaWOtwA\nwD/VNtxVtbOqzq+q83dfev3U4wDAQrQN9xhj1xhjxxhjx7G32zb1OACwEG3DDQA3R8INAI0INwA0\nItwA0MiyX4DlaVPPAADLxIobABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4A\naES4AaAR4QaARoQbABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR\n4QaARoQbABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARrZP\nPcCh8NeXH5uT3v3sqcdYTa+beoDVdbcXnjv1CEBDVtwA0IhwA0Ajwg0AjQg3ADQi3ADQiHADQCPC\nDQCNCDcANCLcANCIcANAI8INAI0INwA0ItwA0IhwA0Ajwg0AjQg3ADQi3ADQiHADQCPCDQCNCDcA\nNCLcANCIcANAI8INAI0INwA0ItwA0IhwA0Ajwg0AjQg3ADQi3ADQiHADQCPCDQCNCDcANCLcANCI\ncANAI8INAI0INwA0ItwA0IhwA0Ajwg0AjSxluKvqnKo6beo5AGDZLGW4AYCN7TPcVfXYqvpWVW2f\nf373qhpV9ZtrHvPyqvpgVW2rqrdV1Rer6pqq+lxVvbiqDlvz2NOr6v1V9byquqiqLq+qd1TVUTfc\nn+QRSZ47/++MqjrxEH/fANDS9v14zIeT3DLJjiTnJnlkkkuSPGrNYx6Z5I8y+0XgoiRPSrI7yclJ\ndiW5NMnb1jz+YUm+nuSUJCckeU+SzyZ5RZLnJblHkr9N8p/mj999gN8XAKykfa64xxhXJvlkbgz1\nI5OcluQuVXWH+Ur5QUnOGWPsGWO8dIzx8THGl8YY70ny1iQ/ue6v/WaS54wx/maM8adJ/keSR8//\ne1ck+XaSq8cY35jfrl8/V1XtrKrzq+r866+8ajPfOwC0s7/PcZ+TWbCT2WHs/53kvPm2hybZM/88\nVfXseVB3V9WVSV6Q5M7r/r4LxhjXrfn8a0lufyCDjzF2jTF2jDF2bDv6VgfypQDQ1oGE+6FVda8k\nt07yifm2R2UW778cY+ypqp9I8oYkpyc5Ncn9k7wlyRHr/r496z4fBzALANxs7c9z3Mnsee5bJHlx\nkr8YY1xfVedk9vz1xZk9v50kP5TkY2OM776Uq6pO2sRc306ybRNfBwArbb9WuWue535KkrPnmz+a\n2YllD85s9Z3MTjB7wPxM9LtX1UsyO7R+oL6U5OSqOrGqjll7VjoA3JwdSBDPzmwVfE6SjDH+MbOz\nzK/N/PntJL+V2Rni70ry8SQnJnntJuZ6TWar7gsyO6N8/XPkAHCzVGOMqWc4aLe48wnj+Bc9f+ox\nVlP/H4+ldbcXnjv1CMAS+dA48xNjjB37epxD0ADQiHADQCPCDQCNCDcANCLcANCIcANAI8INAI0I\nNwA0ItwA0IhwA0Ajwg0AjQg3ADQi3ADQiHADQCPCDQCNCDcANCLcANCIcANAI8INAI0INwA0ItwA\n0IhwA0Ajwg0AjQg3ADQi3ADQiHADQCPCDQCNCDcANCLcANCIcANAI8INAI0INwA0ItwA0IhwA0Aj\nwg0AjQg3ADSyfeoBDoVt1yS3/Zuaegw4IJfsfMjUI6ysY3Z9dOoRYMtYcQNAI8INAI0INwA0ItwA\n0IhwA0Ajwg0AjQg3ADQi3ADQiHADQCPCDQCNCDcANCLcANCIcANAI8INAI0INwA0ItwA0IhwA0Aj\nwg0AjQg3ADQi3ADQiHADQCPCDQCNCDcANCLcANCIcANAI8INAI0INwA0ItwA0IhwA0Ajwg0AjQg3\nADQi3ADQiHADQCPCDQCNCDcANCLcANCIcANAI8INAI0INwA0ItwA0IhwA0AjSxXuqnpMVX24qi6v\nqsuq6k+q6p5TzwUAy2Kpwp3kVknekOTkJI9MckWS91XVEVMOBQDLYvvUA6w1xvj9tZ9X1U8n+WZm\nIf+LdfftTLIzSQ4/+p8takQAmNRSrbir6qSqeldVfb6qvpnkHzKb8c7rHzvG2DXG2DHG2LH9lrda\n+KwAMIWlWnEneV+Si5I8a/7ndUkuSOJQOQBkicJdVbdLcs8kzx1jnD3f9oAs0YwAMLVliuLlSS5J\n8syqujDJHZO8OrNVNwCQJXqOe4zxnSQ/keS+ST6T5M1JXpLk2innAoBlskwr7owxzkryL9dtPnqK\nWQBgGS3NihsA2DfhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBo\nRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBoRLgBoBHh\nBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBoZPvUAxwKh12XHLX7\nO1OPsZJqjKlHgAN2zeNPnnqElXXkH5439Qg3e1bcANCIcANAI8INAI0INwA0ItwA0IhwA0Ajwg0A\njQg3ADQi3ADQiHADQCPCDQCNCDcANCLcANCIcANAI8INAI0INwA0ItwA0IhwA0Ajwg0AjQg3ADQi\n3ADQiHADQCPCDQCNCDcANCLcANCIcANAI8INAI0INwA0ItwA0IhwA0Ajwg0AjQg3ADQi3ADQiHAD\nQCPCDQCNCDcANCLcANCIcANAIwcU7qo6p6pO26phAIC9s+IGgEaWPtxVdcTUMwDAsthMuLdX1Rur\n6vL57dVVdVgyi2xVvbKqvlpVV1XVx6vq1LVfXFX3qqoPVNW3quriqvrdqjpuzf2nV9X7q+oXquqr\nSb56cN8iAKyOzYT7yfOve0iSZyXZmeT58/vekeQRSX4qyX2SvDPJ+6rqfklSVXdI8udJPpPk5CSn\nJDk6yXtviP/cI5LcN8ljkjx6EzMCwEravomv+XqSnxtjjCR/W1X3SPLCqvpfSX4yyYljjK/MH3ta\nVZ2SWeB/JslzkvzVGOMXbvjLqurfJrksyY4k5803/2OSp48xrr2pIapqZ2a/NOSII2+7iW8DAPrZ\nzIr73Hm0b/DRJHdM8kNJKskFVXXlDbckP5rkpPljH5jk4evuv3B+30lr/s7P7C3aSTLG2DXG2DHG\n2HH4LY7exLcBAP1sZsW9NyPJg5LsWbf9mvmfhyX5QJKf3+Br/2HNx1cd4rkAYCVsJtwPrqpas+r+\ngSRfy2zlXUmOG2OcfRNf+8kkT0ry5THG+rgDAPuwmUPlxyd5Q1V9f1U9Mcl/TPL6McZnk5yR5PSq\nemJV3bWqdlTVz1fVE+Zf++Yk35Pk3VX14PljTqmqXVV160PyHQHACtvMivuMJNuSfCyzQ+NvS/L6\n+X0/neSXk7wqyZ0yO+nsvCRnJ8kY42tV9dAkr0jyx0lumeQrSf40yV6f0wYADjDcY4xHrvn0Zze4\nf0+Sl81vN/V3fC7JE/dy/9MOZCYAuDlZ+iunAQA3Em4AaES4AaAR4QaARoQbABoRbgBoRLgBoBHh\nBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBoRLgBoBHhBoBGhBsA\nGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQbABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhE\nuAGgEeEGgEa2Tz3AoVDXjxxxxXVTjwGw8q774QdOPcLq+rMz9+thVtwA0IhwA0Ajwg0AjQg3ADQi\n3ADQiHADQCPCDQCNCDcANCLcANCIcANAI8INAI0INwA0ItwA0IhwA0Ajwg0AjQg3ADQi3ADQiHAD\nQCPCDQCNCDcANCLcANCIcANAI8INAI0INwA0ItwA0IhwA0Ajwg0AjQg3ADQi3ADQiHADQCPCDQCN\nCDcANCLcANCIcANAI8INAI0INwA0ItwA0IhwA0Ajwg0AjSxNuKvq9KoaG9zOnXo2AFgW26ceYJ0P\nJXnqum3fnmIQAFhGyxbua8cY35h6CABYVktzqBwA2LdlC/djqurKdbdXbvTAqtpZVedX1fl79ly1\n6DkBYBLLdqj8z5PsXLft/230wDHGriS7kuTWt7nT2OK5AGApLFu4rx5j/P3UQwDAslq2Q+UAwF4s\n24r7FlV13Lpt148xdk8yDQAsmWUL9ylJvr5u20VJ7jTBLACwdJbmUPkY42ljjNrgJtoAMLc04QYA\n9k24AaAR4QaARoQbABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR\n4QaARoQbABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaEW4AaES4AaAR4QaARoQb\nABoRbgBoRLgBoBHhBoBGhBsAGhFuAGhEuAGgEeEGgEaEGwAaqTHG1DMctKraneTLU8+xn45JcsnU\nQ6wo+3br2Ldbx77dOt327V3GGMfu60ErEe5Oqur8McaOqedYRfbt1rFvt459u3VWdd86VA4AjQg3\nADQi3Iu3a+oBVph9u3Xs261j326dldy3nuMGgEasuAGgEeEGgEaEGwAaEW4AaES4AaCR/w8HQrsI\n5PfN9QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27ff1c02668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first head of last state dec_enc_attns\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe4AAAIACAYAAABEsY85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27fbd705908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%\n",
    "# code by Tae Hwan Jung(Jeff Jung) @graykode, Derek Miller @dmmiller612\n",
    "# Reference : https://github.com/jadore801120/attention-is-all-you-need-pytorch\n",
    "#           https://github.com/JayParks/transformer\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# S: Symbol that shows starting of decoding input\n",
    "# E: Symbol that shows starting of decoding output\n",
    "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
    "\n",
    "def make_batch(sentences):\n",
    "    input_batch = [[src_vocab[n] for n in sentences[0].split()]]\n",
    "    output_batch = [[tgt_vocab[n] for n in sentences[1].split()]]\n",
    "    target_batch = [[tgt_vocab[n] for n in sentences[2].split()]]\n",
    "    return torch.LongTensor(input_batch), torch.LongTensor(output_batch), torch.LongTensor(target_batch)\n",
    "\n",
    "def get_sinusoid_encoding_table(n_position, d_model):\n",
    "    def cal_angle(position, hid_idx):\n",
    "        return position / np.power(10000, 2 * (hid_idx // 2) / d_model)\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, hid_j) for hid_j in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
    "    return torch.FloatTensor(sinusoid_table)\n",
    "\n",
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k\n",
    "\n",
    "def get_attn_subsequent_mask(seq):\n",
    "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1)\n",
    "    subsequent_mask = torch.from_numpy(subsequent_mask).byte()\n",
    "    return subsequent_mask\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "        self.linear = nn.Linear(n_heads * d_v, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
    "        output = self.linear(context)\n",
    "        return self.layer_norm(output + residual), attn # output: [batch_size x len_q x d_model]\n",
    "\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        residual = inputs # inputs : [batch_size, len_q, d_model]\n",
    "        output = nn.ReLU()(self.conv1(inputs.transpose(1, 2)))\n",
    "        output = self.conv2(output).transpose(1, 2)\n",
    "        return self.layer_norm(output + residual)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
    "        return enc_outputs, attn\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.dec_self_attn = MultiHeadAttention()\n",
    "        self.dec_enc_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n",
    "        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)\n",
    "        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
    "        dec_outputs = self.pos_ffn(dec_outputs)\n",
    "        return dec_outputs, dec_self_attn, dec_enc_attn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.src_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(src_len+1, d_model),freeze=True)\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, enc_inputs): # enc_inputs : [batch_size x source_len]\n",
    "        enc_outputs = self.src_emb(enc_inputs) + self.pos_emb(torch.LongTensor([[1,2,3,4,0]]))\n",
    "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)\n",
    "        enc_self_attns = []\n",
    "        for layer in self.layers:\n",
    "            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)\n",
    "            enc_self_attns.append(enc_self_attn)\n",
    "        return enc_outputs, enc_self_attns\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(tgt_len+1, d_model),freeze=True)\n",
    "        self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, dec_inputs, enc_inputs, enc_outputs): # dec_inputs : [batch_size x target_len]\n",
    "        dec_outputs = self.tgt_emb(dec_inputs) + self.pos_emb(torch.LongTensor([[5,1,2,3,4]]))\n",
    "        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs)\n",
    "        dec_self_attn_subsequent_mask = get_attn_subsequent_mask(dec_inputs)\n",
    "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequent_mask), 0)\n",
    "\n",
    "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)\n",
    "\n",
    "        dec_self_attns, dec_enc_attns = [], []\n",
    "        for layer in self.layers:\n",
    "            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
    "            dec_self_attns.append(dec_self_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "        return dec_outputs, dec_self_attns, dec_enc_attns\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False)\n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        enc_outputs, enc_self_attns = self.encoder(enc_inputs)\n",
    "        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
    "        dec_logits = self.projection(dec_outputs) # dec_logits : [batch_size x src_vocab_size x tgt_vocab_size]\n",
    "        return dec_logits.view(-1, dec_logits.size(-1)), enc_self_attns, dec_self_attns, dec_enc_attns\n",
    "\n",
    "def showgraph(attn):\n",
    "    attn = attn[-1].squeeze(0)[0]\n",
    "    attn = attn.squeeze(0).data.numpy()\n",
    "    fig = plt.figure(figsize=(n_heads, n_heads)) # [n_heads, n_heads]\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attn, cmap='viridis')\n",
    "    ax.set_xticklabels(['']+sentences[0].split(), fontdict={'fontsize': 14}, rotation=90)\n",
    "    ax.set_yticklabels(['']+sentences[2].split(), fontdict={'fontsize': 14})\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E']\n",
    "\n",
    "    # Transformer Parameters\n",
    "    # Padding Should be Zero\n",
    "    src_vocab = {'P': 0, 'ich': 1, 'mochte': 2, 'ein': 3, 'bier': 4}\n",
    "    src_vocab_size = len(src_vocab)\n",
    "\n",
    "    tgt_vocab = {'P': 0, 'i': 1, 'want': 2, 'a': 3, 'beer': 4, 'S': 5, 'E': 6}\n",
    "    number_dict = {i: w for i, w in enumerate(tgt_vocab)}\n",
    "    tgt_vocab_size = len(tgt_vocab)\n",
    "\n",
    "    src_len = 5 # length of source\n",
    "    tgt_len = 5 # length of target\n",
    "\n",
    "    d_model = 512  # Embedding Size\n",
    "    d_ff = 2048  # FeedForward dimension\n",
    "    d_k = d_v = 64  # dimension of K(=Q), V\n",
    "    n_layers = 6  # number of Encoder of Decoder Layer\n",
    "    n_heads = 8  # number of heads in Multi-Head Attention\n",
    "\n",
    "    model = Transformer()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    enc_inputs, dec_inputs, target_batch = make_batch(sentences)\n",
    "\n",
    "    for epoch in range(500):\n",
    "        optimizer.zero_grad()\n",
    "        outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)\n",
    "        loss = criterion(outputs, target_batch.contiguous().view(-1))\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Test\n",
    "    predict, _, _, _ = model(enc_inputs, dec_inputs)\n",
    "    predict = predict.data.max(1, keepdim=True)[1]\n",
    "    print(sentences[0], '->', [number_dict[n.item()] for n in predict.squeeze()])\n",
    "\n",
    "    print('first head of last state enc_self_attns')\n",
    "    showgraph(enc_self_attns)\n",
    "\n",
    "    print('first head of last state dec_self_attns')\n",
    "    showgraph(dec_self_attns)\n",
    "\n",
    "    print('first head of last state dec_enc_attns')\n",
    "    showgraph(dec_enc_attns)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
